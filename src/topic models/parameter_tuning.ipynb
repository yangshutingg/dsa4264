{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:32:48.910188Z",
     "iopub.status.busy": "2024-11-11T10:32:48.909809Z",
     "iopub.status.idle": "2024-11-11T10:33:10.553669Z",
     "shell.execute_reply": "2024-11-11T10:33:10.552623Z",
     "shell.execute_reply.started": "2024-11-11T10:32:48.910151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas bertopic keybert gensim sentence-transformers scikit-learn hdbscan\n",
    "#!pip install bertopic==0.16.0 keybert==0.7.0 hdbscan==0.8.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:36:22.480864Z",
     "iopub.status.busy": "2024-11-11T10:36:22.479600Z",
     "iopub.status.idle": "2024-11-11T10:36:22.485168Z",
     "shell.execute_reply": "2024-11-11T10:36:22.484185Z",
     "shell.execute_reply.started": "2024-11-11T10:36:22.480821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:36:56.310895Z",
     "iopub.status.busy": "2024-11-11T10:36:56.310440Z",
     "iopub.status.idle": "2024-11-11T10:36:56.316101Z",
     "shell.execute_reply": "2024-11-11T10:36:56.315039Z",
     "shell.execute_reply.started": "2024-11-11T10:36:56.310854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable HuggingFace tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:36:59.280709Z",
     "iopub.status.busy": "2024-11-11T10:36:59.280317Z",
     "iopub.status.idle": "2024-11-11T10:38:01.959960Z",
     "shell.execute_reply": "2024-11-11T10:38:01.958859Z",
     "shell.execute_reply.started": "2024-11-11T10:36:59.280673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/combined_data_scores.csv')\n",
    "# use line below to load the data if using Kaggle\n",
    "#df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "df = df[['text', 'yearmonth', 'title', 'index', 'average_toxicity_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of BERTopic on sample of data using Coherence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `yearmonth` = `2023-10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df[df['yearmonth'] == '2023-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:39:01.062049Z",
     "iopub.status.busy": "2024-11-11T10:39:01.061688Z",
     "iopub.status.idle": "2024-11-11T10:40:26.112445Z",
     "shell.execute_reply": "2024-11-11T10:40:26.111410Z",
     "shell.execute_reply.started": "2024-11-11T10:39:01.062017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create representation model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Use the representation model in BERTopic on top of the default pipeline\n",
    "topic_model = BERTopic(representation_model=representation_model, nr_topics='auto')\n",
    "\n",
    "# Fit the model on text data\n",
    "topics, probabilities = topic_model.fit_transform(df_sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:40:32.719955Z",
     "iopub.status.busy": "2024-11-11T10:40:32.719531Z",
     "iopub.status.idle": "2024-11-11T10:40:32.760496Z",
     "shell.execute_reply": "2024-11-11T10:40:32.759570Z",
     "shell.execute_reply.started": "2024-11-11T10:40:32.719913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>11293</td>\n",
       "      <td>-1_money_because_life_singapore</td>\n",
       "      <td>[money, because, life, singapore, say, think, ...</td>\n",
       "      <td>[&gt; Why must Holy and unholy be defined  the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3461</td>\n",
       "      <td>0_gaza_palestinians_hamas_palestinian</td>\n",
       "      <td>[gaza, palestinians, hamas, palestinian, pales...</td>\n",
       "      <td>[I dont see why people keep having the mindset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>716</td>\n",
       "      <td>1_cyclist_cyclists_roads_traffic</td>\n",
       "      <td>[cyclist, cyclists, roads, traffic, lanes, cyc...</td>\n",
       "      <td>[&gt;Actually, this is a huge problem because of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>652</td>\n",
       "      <td>2_bullying_bullied_bully_offensive</td>\n",
       "      <td>[bullying, bullied, bully, offensive, revenge,...</td>\n",
       "      <td>[That is what I’m saying though, that I’ve “ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>435</td>\n",
       "      <td>3_rice_chicken_meals_meal</td>\n",
       "      <td>[rice, chicken, meals, meal, food, meat, eatin...</td>\n",
       "      <td>[Chicken rice. My go to post gym meal., Rather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>326</td>\n",
       "      <td>4_income_salary_earn_earning</td>\n",
       "      <td>[income, salary, earn, earning, 5k, rich, 4k, ...</td>\n",
       "      <td>[Yeah i agree with the stats being skewed. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>240</td>\n",
       "      <td>5_prices_affordable_btos_afford</td>\n",
       "      <td>[prices, affordable, btos, afford, cost, bto, ...</td>\n",
       "      <td>[Latest BTO Prices from $364,000 to $509,000 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>228</td>\n",
       "      <td>6_depression_psychiatric_psychiatry_illnesses</td>\n",
       "      <td>[depression, psychiatric, psychiatry, illnesse...</td>\n",
       "      <td>[Is her mental health ok? Could she be dealing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>211</td>\n",
       "      <td>7_pap_paps_papbut_pappies</td>\n",
       "      <td>[pap, paps, papbut, pappies, paiseh, pa, pappy...</td>\n",
       "      <td>[Vote for pap, PAP so good., PAP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>203</td>\n",
       "      <td>8_wipe_wiping_wipes_cleaning</td>\n",
       "      <td>[wipe, wiping, wipes, cleaning, tables, cleanl...</td>\n",
       "      <td>[Need to clear table after eating. In future, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>172</td>\n",
       "      <td>9_police_report_cops_incident</td>\n",
       "      <td>[police, report, cops, incident, cctvs, cctv, ...</td>\n",
       "      <td>[Make a police report, Report to police, Why n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>163</td>\n",
       "      <td>10_foreigners_migrants_migrant_immigrants</td>\n",
       "      <td>[foreigners, migrants, migrant, immigrants, fo...</td>\n",
       "      <td>[That place cannot hire foreigners, I think., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>11_feminism_feminist_feminists_genders</td>\n",
       "      <td>[feminism, feminist, feminists, genders, gende...</td>\n",
       "      <td>[you have some really solid points! just to no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>12_thanks_thank_welcome_cheers</td>\n",
       "      <td>[thanks, thank, welcome, cheers, anytime, appr...</td>\n",
       "      <td>[Thanks, Thanks, Thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>143</td>\n",
       "      <td>13_buddhists_buddhism_religions_violence</td>\n",
       "      <td>[buddhists, buddhism, religions, violence, pea...</td>\n",
       "      <td>[I don’t know enough of all religions to make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>142</td>\n",
       "      <td>14_yes_yep_no_nope</td>\n",
       "      <td>[yes, yep, no, nope, yessir, yeah, yea, absolu...</td>\n",
       "      <td>[Yes, Yes, Yes and no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>136</td>\n",
       "      <td>15_his_bf_relationship_him</td>\n",
       "      <td>[his, bf, relationship, him, himself, boyfrien...</td>\n",
       "      <td>[I think, if you feel you have the maturity to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>134</td>\n",
       "      <td>16_opposition_oppositions_voting_vote</td>\n",
       "      <td>[opposition, oppositions, voting, vote, opposi...</td>\n",
       "      <td>[Vote opposition, Vote opposition, Vote opposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "      <td>17_sinkie_sinkies_sinkieland_sinking</td>\n",
       "      <td>[sinkie, sinkies, sinkieland, sinking, pappy, ...</td>\n",
       "      <td>[Sinkie pwn sinkie is a real thing here. Alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>123</td>\n",
       "      <td>18_resignation_leave_resign_leaving</td>\n",
       "      <td>[resignation, leave, resign, leaving, resigned...</td>\n",
       "      <td>[I dont think employers care about employees o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                           Name  \\\n",
       "0      -1  11293                -1_money_because_life_singapore   \n",
       "1       0   3461          0_gaza_palestinians_hamas_palestinian   \n",
       "2       1    716               1_cyclist_cyclists_roads_traffic   \n",
       "3       2    652             2_bullying_bullied_bully_offensive   \n",
       "4       3    435                      3_rice_chicken_meals_meal   \n",
       "5       4    326                   4_income_salary_earn_earning   \n",
       "6       5    240                5_prices_affordable_btos_afford   \n",
       "7       6    228  6_depression_psychiatric_psychiatry_illnesses   \n",
       "8       7    211                      7_pap_paps_papbut_pappies   \n",
       "9       8    203                   8_wipe_wiping_wipes_cleaning   \n",
       "10      9    172                  9_police_report_cops_incident   \n",
       "11     10    163      10_foreigners_migrants_migrant_immigrants   \n",
       "12     11    153         11_feminism_feminist_feminists_genders   \n",
       "13     12    151                 12_thanks_thank_welcome_cheers   \n",
       "14     13    143       13_buddhists_buddhism_religions_violence   \n",
       "15     14    142                             14_yes_yep_no_nope   \n",
       "16     15    136                     15_his_bf_relationship_him   \n",
       "17     16    134          16_opposition_oppositions_voting_vote   \n",
       "18     17    134           17_sinkie_sinkies_sinkieland_sinking   \n",
       "19     18    123            18_resignation_leave_resign_leaving   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [money, because, life, singapore, say, think, ...   \n",
       "1   [gaza, palestinians, hamas, palestinian, pales...   \n",
       "2   [cyclist, cyclists, roads, traffic, lanes, cyc...   \n",
       "3   [bullying, bullied, bully, offensive, revenge,...   \n",
       "4   [rice, chicken, meals, meal, food, meat, eatin...   \n",
       "5   [income, salary, earn, earning, 5k, rich, 4k, ...   \n",
       "6   [prices, affordable, btos, afford, cost, bto, ...   \n",
       "7   [depression, psychiatric, psychiatry, illnesse...   \n",
       "8   [pap, paps, papbut, pappies, paiseh, pa, pappy...   \n",
       "9   [wipe, wiping, wipes, cleaning, tables, cleanl...   \n",
       "10  [police, report, cops, incident, cctvs, cctv, ...   \n",
       "11  [foreigners, migrants, migrant, immigrants, fo...   \n",
       "12  [feminism, feminist, feminists, genders, gende...   \n",
       "13  [thanks, thank, welcome, cheers, anytime, appr...   \n",
       "14  [buddhists, buddhism, religions, violence, pea...   \n",
       "15  [yes, yep, no, nope, yessir, yeah, yea, absolu...   \n",
       "16  [his, bf, relationship, him, himself, boyfrien...   \n",
       "17  [opposition, oppositions, voting, vote, opposi...   \n",
       "18  [sinkie, sinkies, sinkieland, sinking, pappy, ...   \n",
       "19  [resignation, leave, resign, leaving, resigned...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [> Why must Holy and unholy be defined  the sa...  \n",
       "1   [I dont see why people keep having the mindset...  \n",
       "2   [>Actually, this is a huge problem because of ...  \n",
       "3   [That is what I’m saying though, that I’ve “ma...  \n",
       "4   [Chicken rice. My go to post gym meal., Rather...  \n",
       "5   [Yeah i agree with the stats being skewed. But...  \n",
       "6   [Latest BTO Prices from $364,000 to $509,000 3...  \n",
       "7   [Is her mental health ok? Could she be dealing...  \n",
       "8                   [Vote for pap, PAP so good., PAP]  \n",
       "9   [Need to clear table after eating. In future, ...  \n",
       "10  [Make a police report, Report to police, Why n...  \n",
       "11  [That place cannot hire foreigners, I think., ...  \n",
       "12  [you have some really solid points! just to no...  \n",
       "13                           [Thanks, Thanks, Thanks]  \n",
       "14  [I don’t know enough of all religions to make ...  \n",
       "15                             [Yes, Yes, Yes and no]  \n",
       "16  [I think, if you feel you have the maturity to...  \n",
       "17  [Vote opposition, Vote opposition, Vote opposi...  \n",
       "18  [Sinkie pwn sinkie is a real thing here. Alway...  \n",
       "19  [I dont think employers care about employees o...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 20 most frequent topics\n",
    "topic_model.get_topic_info().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:40:47.146858Z",
     "iopub.status.busy": "2024-11-11T10:40:47.145952Z",
     "iopub.status.idle": "2024-11-11T10:41:44.168934Z",
     "shell.execute_reply": "2024-11-11T10:41:44.167843Z",
     "shell.execute_reply.started": "2024-11-11T10:40:47.146813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.33345405750800106\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_docs = topic_model._preprocess_text(df_sample['text'])\n",
    "\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Tokenize the documents\n",
    "tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "\n",
    "# Create Gensim dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Get the top topic words for each topic from BERTopic\n",
    "topic_words = [[word for word, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Convert topic words to token IDs using the Gensim dictionary\n",
    "topic_ids = [[dictionary.token2id[word] for word in topic if word in dictionary.token2id] \n",
    "             for topic in topic_words]\n",
    "\n",
    "# Evaluate coherence score\n",
    "coherence_model = CoherenceModel(topics=topic_ids, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "\n",
    "# Get coherence score\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Improvemnt through Manual Data Cleaning and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T10:42:04.976455Z",
     "iopub.status.busy": "2024-11-11T10:42:04.976068Z",
     "iopub.status.idle": "2024-11-11T10:42:16.539319Z",
     "shell.execute_reply": "2024-11-11T10:42:16.538011Z",
     "shell.execute_reply.started": "2024-11-11T10:42:04.976417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T11:03:33.496162Z",
     "iopub.status.busy": "2024-11-11T11:03:33.495360Z",
     "iopub.status.idle": "2024-11-11T11:03:33.501843Z",
     "shell.execute_reply": "2024-11-11T11:03:33.500784Z",
     "shell.execute_reply.started": "2024-11-11T11:03:33.496120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from umap import UMAP\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `preprocess_text()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T11:03:36.871776Z",
     "iopub.status.busy": "2024-11-11T11:03:36.871001Z",
     "iopub.status.idle": "2024-11-11T11:03:36.958845Z",
     "shell.execute_reply": "2024-11-11T11:03:36.957943Z",
     "shell.execute_reply.started": "2024-11-11T11:03:36.871733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "\n",
      "NLTK data paths: ['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "def setup_nltk_in_kaggle():\n",
    "    \"\"\"\n",
    "    Setup NLTK specifically for Kaggle environment\n",
    "    \"\"\"\n",
    "    # Create directory in the Kaggle working directory\n",
    "    nltk_data_dir = Path('/kaggle/working/nltk_data')\n",
    "    nltk_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set the NLTK_DATA environment variable\n",
    "    os.environ['NLTK_DATA'] = str(nltk_data_dir)\n",
    "    \n",
    "    # Add the path to NLTK's search paths\n",
    "    nltk.data.path.append(str(nltk_data_dir))\n",
    "    \n",
    "    # Download required resources\n",
    "    print(\"Downloading NLTK resources...\")\n",
    "    for resource in ['wordnet', 'stopwords', 'omw-1.4']:\n",
    "        try:\n",
    "            nltk.download(resource, download_dir=str(nltk_data_dir), quiet=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {resource}: {e}\")\n",
    "    \n",
    "    print(\"\\nNLTK data paths:\", nltk.data.path)\n",
    "    \n",
    "    # Alternative preprocessing function that doesn't rely on WordNet\n",
    "    def preprocess_text(texts):\n",
    "        \"\"\"\n",
    "        Preprocess text without relying on WordNet lemmatization\n",
    "        \"\"\"\n",
    "        # Basic English stop words\n",
    "        stop_words = set([\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "            'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "            'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "            'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "            'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "            'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "            'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "            'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "            'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "            'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "            'then', 'once'\n",
    "        ])\n",
    "        \n",
    "        def clean_text(text):\n",
    "            # Convert to string and lowercase\n",
    "            text = str(text).lower()\n",
    "            # Remove special characters and extra whitespace\n",
    "            text = ' '.join(''.join(c if c.isalnum() else ' ' for c in text).split())\n",
    "            # Remove stop words and short tokens\n",
    "            tokens = [token for token in text.split() if token not in stop_words and len(token) > 2]\n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        return [clean_text(text) for text in texts]\n",
    "    \n",
    "    return preprocess_text\n",
    "\n",
    "# Set up NLTK and get the preprocessing function\n",
    "preprocess_text = setup_nltk_in_kaggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise BERTopic model with parameters `min_df`, `max_df`, `ngram_range`, `n_neighbors`, `n_components` and `min_dist` tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T11:21:40.270518Z",
     "iopub.status.busy": "2024-11-11T11:21:40.270074Z",
     "iopub.status.idle": "2024-11-11T11:23:08.749144Z",
     "shell.execute_reply": "2024-11-11T11:23:08.748024Z",
     "shell.execute_reply.started": "2024-11-11T11:21:40.270477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Topics:\n",
      "    Topic  Count                                              Name  \\\n",
      "0      -1  13177  -1_singaporeans_countries_government_singaporean   \n",
      "1       0   1541                               0_yes_yep_yeah_nope   \n",
      "2       1   1479     1_singapore_singaporeans_singaporean_malaysia   \n",
      "3       2   1352                   2_peace_war_violence_supporting   \n",
      "4       3    773                   3_affordable_afford_prices_loan   \n",
      "5       4    738            4_bus_public transport_driving_vehicle   \n",
      "6       5    731       5_advertising_marketing_reviews_influencers   \n",
      "7       6    729                   6_police_report_arrest_arrested   \n",
      "8       7    599                        7_sinkie_sinkies_rice_meal   \n",
      "9       8    595                           8_ceca_cai_siao_cai fan   \n",
      "10      9    522                          9_women_gender_men_woman   \n",
      "11     10    348                10_patients_doctors_medical_clinic   \n",
      "12     11    316                      11_ugly_attractive_face_look   \n",
      "13     12    222                       12_pap_mps_parliament_party   \n",
      "14     13    206                        13_gay_sexual_straight_sex   \n",
      "15     14    206               14_leave_leaving_workplace_employee   \n",
      "16     15    183                       15_tall_height_shorter_size   \n",
      "17     16    164                           16_run_kill_die_die die   \n",
      "18     17    153                         17_sheep_animals_dogs_dog   \n",
      "19     18    145                       18_born_birth_late 20s_aged   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [singaporeans, countries, government, singapor...   \n",
      "1   [yes, yep, yeah, nope, yup, said yes, yeap, na...   \n",
      "2   [singapore, singaporeans, singaporean, malaysi...   \n",
      "3   [peace, war, violence, supporting, support, la...   \n",
      "4   [affordable, afford, prices, loan, housing, es...   \n",
      "5   [bus, public transport, driving, vehicle, driv...   \n",
      "6   [advertising, marketing, reviews, influencers,...   \n",
      "7   [police, report, arrest, arrested, crime, offi...   \n",
      "8   [sinkie, sinkies, rice, meal, chicken, food, m...   \n",
      "9   [ceca, cai, siao, cai fan, sia, learn, don, ti...   \n",
      "10  [women, gender, men, woman, female, girls, dat...   \n",
      "11  [patients, doctors, medical, clinic, doctor, m...   \n",
      "12  [ugly, attractive, face, look, facial, eyes, n...   \n",
      "13  [pap, mps, parliament, party, master, shld, lo...   \n",
      "14  [gay, sexual, straight, sex, horny, incel, par...   \n",
      "15  [leave, leaving, workplace, employee, work, ex...   \n",
      "16  [tall, height, shorter, size, small, length, s...   \n",
      "17  [run, kill, die, die die, finish, cut, end, bu...   \n",
      "18  [sheep, animals, dogs, dog, cat, feeding, rat,...   \n",
      "19  [born, birth, late 20s, aged, young, age group...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [love all funny comments really guys not fair ...  \n",
      "1                                 [yes, yes, yes yes]  \n",
      "2   [just saying chinese longer means first settle...  \n",
      "3   [correct wrong feel like only recently made aw...  \n",
      "4   [latest bto prices 364 000 509 000 room 537 00...  \n",
      "5   [thank bus driver, just hope taxi driver drivi...  \n",
      "6   [paid ads masked genuine reviews pay win never...  \n",
      "7   [made police report, report police, report pol...  \n",
      "8   [sinkie pawn sinkie, rather chicken rice, chic...  \n",
      "9   [tinder lot looking ons know how play game sub...  \n",
      "10  [some really solid points just note points are...  \n",
      "11  [precisely thought only one noticed nothing no...  \n",
      "12  [doesn know boys can learn charisma humour abi...  \n",
      "13                         [pwned pap, pap, vote pap]  \n",
      "14  [sex prostitute just sex woman thats nothing w...  \n",
      "15  [try see there can compromise especially might...  \n",
      "16  [tall short how know answer dont compare other...  \n",
      "17                      [2187 run, how run, just run]  \n",
      "18  [cotton sheep, sheep milked, dogs die suspecte...  \n",
      "19  [one born few months ago year old, didn say bo...  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df = pd.read_csv('../../data/combined_data_scores.csv')\n",
    "# use the line below to load the data if using Kaggle\n",
    "#df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "yearmonth = '2023-10'\n",
    "df_sample = df[df['yearmonth'] == yearmonth]\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_texts = preprocess_text(df_sample['text'])\n",
    "\n",
    "# Initialize BERTopic model\n",
    "representation_model = KeyBERTInspired()\n",
    "vectorizer_model = CountVectorizer(min_df=5,\n",
    "                                   max_df=0.9,\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   stop_words=\"english\")\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=10,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine'\n",
    ")\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    umap_model=umap_model,\n",
    "    representation_model=representation_model,  # Use a transformer model for better coherence\n",
    "    nr_topics=50,                             # Set a target number of topics\n",
    "    min_topic_size=10                          # Minimum documents per topic\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probabilities = topic_model.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Get topic info\n",
    "print(\"\\nTop 20 Topics:\")\n",
    "print(topic_model.get_topic_info().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T11:23:38.303087Z",
     "iopub.status.busy": "2024-11-11T11:23:38.302599Z",
     "iopub.status.idle": "2024-11-11T11:23:42.009811Z",
     "shell.execute_reply": "2024-11-11T11:23:42.008649Z",
     "shell.execute_reply.started": "2024-11-11T11:23:38.303036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating overall coherence score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Coherence Score (C_v): 0.4220\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "def calculate_topic_coherence(topic_model, texts, topics):\n",
    "    \"\"\"\n",
    "    Calculate topic coherence for a BERTopic model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic_model : BERTopic model\n",
    "        The fitted BERTopic model\n",
    "    texts : list\n",
    "        List of preprocessed text documents\n",
    "    topics : list\n",
    "        List of assigned topics from fit_transform\n",
    "    \"\"\"\n",
    "    # Extract vectorizer and build tokenizer\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokens = [tokenizer(doc) for doc in texts]\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    \n",
    "    # Create mapping of word to id\n",
    "    word2id = {word: idx for idx, word in dictionary.items()}\n",
    "    \n",
    "    # Get topic words for each topic\n",
    "    unique_topics = sorted(set(topics) - {-1})  # Exclude -1 topic\n",
    "    topic_words = []\n",
    "    \n",
    "    for topic_idx in unique_topics:\n",
    "        # Get the words for this topic\n",
    "        topic = topic_model.get_topic(topic_idx)\n",
    "        if topic:  # Only include non-empty topics\n",
    "            # Extract just the words (not the weights)\n",
    "            words = [word for word, _ in topic[:10]]  # Take top 10 words per topic\n",
    "            # Convert words to dictionary ids\n",
    "            word_ids = [word2id[word] for word in words if word in word2id]\n",
    "            if word_ids:  # Only add if we have valid words\n",
    "                topic_words.append(word_ids)\n",
    "    \n",
    "    # Calculate coherence only if we have valid topics\n",
    "    if topic_words:\n",
    "        try:\n",
    "            coherence_model = CoherenceModel(\n",
    "                topics=topic_words,\n",
    "                texts=tokens,\n",
    "                dictionary=dictionary,\n",
    "                coherence='c_v'\n",
    "            )\n",
    "            return coherence_model.get_coherence()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating coherence: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def calculate_per_topic_coherence(topic_model, texts, topics):\n",
    "    \"\"\"\n",
    "    Calculate coherence scores for each individual topic\n",
    "    \"\"\"\n",
    "    # Extract vectorizer and build tokenizer\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokens = [tokenizer(doc) for doc in texts]\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    word2id = {word: idx for idx, word in dictionary.items()}\n",
    "    \n",
    "    unique_topics = sorted(set(topics) - {-1})  # Exclude -1 topic\n",
    "    per_topic_coherence = {}\n",
    "    \n",
    "    for topic_idx in unique_topics:\n",
    "        # Get the words for this topic\n",
    "        topic = topic_model.get_topic(topic_idx)\n",
    "        if topic:\n",
    "            # Extract just the words (not the weights)\n",
    "            words = [word for word, _ in topic[:10]]  # Take top 10 words\n",
    "            # Convert words to dictionary ids\n",
    "            word_ids = [word2id[word] for word in words if word in word2id]\n",
    "            if word_ids:\n",
    "                try:\n",
    "                    coherence_model = CoherenceModel(\n",
    "                        topics=[word_ids],\n",
    "                        texts=tokens,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v'\n",
    "                    )\n",
    "                    per_topic_coherence[topic_idx] = coherence_model.get_coherence()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating coherence for topic {topic_idx}: {e}\")\n",
    "                    per_topic_coherence[topic_idx] = None\n",
    "    \n",
    "    return per_topic_coherence\n",
    "\n",
    "# Calculate coherence score\n",
    "print(\"Calculating overall coherence score...\")\n",
    "coherence_score = calculate_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "\n",
    "if coherence_score is not None:\n",
    "    print(f\"\\nTopic Coherence Score (C_v): {coherence_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate overall coherence score\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5935933,
     "sourceId": 9705720,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5936031,
     "sourceId": 9705856,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5939253,
     "sourceId": 9710067,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
