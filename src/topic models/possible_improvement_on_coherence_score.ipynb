{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:43:11.294722Z",
     "iopub.status.busy": "2024-10-26T18:43:11.294332Z",
     "iopub.status.idle": "2024-10-26T18:43:53.260849Z",
     "shell.execute_reply": "2024-10-26T18:43:53.259847Z",
     "shell.execute_reply.started": "2024-10-26T18:43:11.294684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:44:05.014151Z",
     "iopub.status.busy": "2024-10-26T18:44:05.013281Z",
     "iopub.status.idle": "2024-10-26T18:44:05.018086Z",
     "shell.execute_reply": "2024-10-26T18:44:05.017232Z",
     "shell.execute_reply.started": "2024-10-26T18:44:05.014109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable HuggingFace tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:44:07.721403Z",
     "iopub.status.busy": "2024-10-26T18:44:07.720415Z",
     "iopub.status.idle": "2024-10-26T18:45:17.593626Z",
     "shell.execute_reply": "2024-10-26T18:45:17.592789Z",
     "shell.execute_reply.started": "2024-10-26T18:44:07.721351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "df = df[['text', 'yearmonth', 'title', 'index', 'average_toxicity_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_sample = df[df['yearmonth'] == '2022-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create representation model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Use the representation model in BERTopic on top of the default pipeline\n",
    "topic_model = BERTopic(representation_model=representation_model, nr_topics='auto')\n",
    "\n",
    "# Fit the model on text data\n",
    "topics, probabilities = topic_model.fit_transform(df_sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_docs = topic_model._preprocess_text(df_sample['text'])\n",
    "\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Tokenize the documents\n",
    "tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "\n",
    "# Create Gensim dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Get the top topic words for each topic from BERTopic\n",
    "topic_words = [[word for word, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Convert topic words to token IDs using the Gensim dictionary\n",
    "topic_ids = [[dictionary.token2id[word] for word in topic if word in dictionary.token2id] \n",
    "             for topic in topic_words]\n",
    "\n",
    "# Evaluate coherence score\n",
    "coherence_model = CoherenceModel(topics=topic_ids, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "\n",
    "# Get coherence score\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvemnt in Data Preprocessing and Coherent Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:45:33.912691Z",
     "iopub.status.busy": "2024-10-26T18:45:33.911896Z",
     "iopub.status.idle": "2024-10-26T18:46:03.338057Z",
     "shell.execute_reply": "2024-10-26T18:46:03.337011Z",
     "shell.execute_reply.started": "2024-10-26T18:45:33.912645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:46:42.567819Z",
     "iopub.status.busy": "2024-10-26T18:46:42.567370Z",
     "iopub.status.idle": "2024-10-26T18:46:42.583001Z",
     "shell.execute_reply": "2024-10-26T18:46:42.581697Z",
     "shell.execute_reply.started": "2024-10-26T18:46:42.567779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define function for text preprocessing\n",
    "def preprocess_text(texts):\n",
    "    # Initialize stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Remove non-alphanumeric characters\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        # Tokenize and lemmatize\n",
    "        tokens = text.lower().split()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    return [clean_text(text) for text in texts]\n",
    "\n",
    "# Function to get topics for each yearmonth\n",
    "def process_topics_by_year(df, year, output_csv, start_month=1):\n",
    "    \"\"\"\n",
    "    Process topics for all yearmonths in the specified year using BERTopic, and output results into a csv file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The original DataFrame containing the data.\n",
    "    - year: The year for which to process the data (e.g., 2022).\n",
    "    - output_csv: The output CSV file to save the results.\n",
    "    - start_month: The starting month for processing (default is 1).\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specified year\n",
    "    df_filtered_year = df[df['yearmonth'].str.startswith(str(year))]\n",
    "\n",
    "    # Get unique year-month combinations for the specified year\n",
    "    unique_yearmonths = df_filtered_year['yearmonth'].unique()\n",
    "\n",
    "    for idx, yearmonth in enumerate(unique_yearmonths):\n",
    "        # Skip months before the start_month\n",
    "        if int(yearmonth[-2:]) < start_month:\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Processing {yearmonth}...\")\n",
    "            \n",
    "            # Filter data for the current year-month\n",
    "            df_filtered = df[df['yearmonth'] == yearmonth]\n",
    "            \n",
    "            # Preprocess the text data\n",
    "            preprocessed_texts = preprocess_text(df_filtered['text'])\n",
    "            \n",
    "            # Initialise representation model\n",
    "            representation_model = KeyBERTInspired()\n",
    "\n",
    "            # Initialize BERTopic model with custom vectorizer and parameters\n",
    "            vectorizer_model = CountVectorizer(min_df=10, stop_words=\"english\")\n",
    "            topic_model = BERTopic(representation_model=representation_model, nr_topics=\"auto\", \n",
    "                                   vectorizer_model=vectorizer_model, min_topic_size=10)\n",
    "\n",
    "            # Fit the model on the preprocessed text data\n",
    "            topics, probabilities = topic_model.fit_transform(preprocessed_texts)\n",
    "\n",
    "            # Save topics per document\n",
    "            df_topics = pd.DataFrame({\n",
    "                'index': df_filtered['index'],\n",
    "                'Topic': topics\n",
    "            })\n",
    "\n",
    "            # Save topic information (such as topic words and frequencies)\n",
    "            topic_info = topic_model.get_topic_info()\n",
    "\n",
    "            # Merge topics with original data\n",
    "            df_combined = pd.merge(df_filtered, df_topics, on='index', how='left')\n",
    "\n",
    "            # Merge topic info with the combined data\n",
    "            df_final = pd.merge(df_combined, topic_info, on='Topic', how='left')\n",
    "\n",
    "            # Write the result to the output CSV file\n",
    "            if idx == 0:\n",
    "                df_final.to_csv(output_csv, index=False, mode='w')  # Write header for the first batch\n",
    "            else:\n",
    "                df_final.to_csv(output_csv, index=False, mode='a', header=False)  # Append mode without header\n",
    "\n",
    "            print(f\"Processing for {yearmonth} is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:46:47.966379Z",
     "iopub.status.busy": "2024-10-26T18:46:47.965997Z",
     "iopub.status.idle": "2024-10-26T18:46:47.972059Z",
     "shell.execute_reply": "2024-10-26T18:46:47.970861Z",
     "shell.execute_reply.started": "2024-10-26T18:46:47.966343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T21:48:29.103983Z",
     "iopub.status.busy": "2024-10-26T21:48:29.103604Z",
     "iopub.status.idle": "2024-10-26T21:48:29.288810Z",
     "shell.execute_reply": "2024-10-26T21:48:29.287909Z",
     "shell.execute_reply.started": "2024-10-26T21:48:29.103948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "\n",
      "NLTK data paths: ['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data', '/kaggle/working/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_nltk_in_kaggle():\n",
    "    \"\"\"\n",
    "    Setup NLTK specifically for Kaggle environment\n",
    "    \"\"\"\n",
    "    # Create directory in the Kaggle working directory\n",
    "    nltk_data_dir = Path('/kaggle/working/nltk_data')\n",
    "    nltk_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set the NLTK_DATA environment variable\n",
    "    os.environ['NLTK_DATA'] = str(nltk_data_dir)\n",
    "    \n",
    "    # Add the path to NLTK's search paths\n",
    "    nltk.data.path.append(str(nltk_data_dir))\n",
    "    \n",
    "    # Download required resources\n",
    "    print(\"Downloading NLTK resources...\")\n",
    "    for resource in ['wordnet', 'stopwords', 'omw-1.4']:\n",
    "        try:\n",
    "            nltk.download(resource, download_dir=str(nltk_data_dir), quiet=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {resource}: {e}\")\n",
    "    \n",
    "    print(\"\\nNLTK data paths:\", nltk.data.path)\n",
    "    \n",
    "    # Alternative preprocessing function that doesn't rely on WordNet\n",
    "    def preprocess_text(texts):\n",
    "        \"\"\"\n",
    "        Preprocess text without relying on WordNet lemmatization\n",
    "        \"\"\"\n",
    "        # Basic English stop words\n",
    "        stop_words = set([\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "            'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "            'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "            'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "            'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "            'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "            'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "            'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "            'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "            'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', \n",
    "            'then', 'once'\n",
    "        ])\n",
    "        \n",
    "        def clean_text(text):\n",
    "            # Convert to string and lowercase\n",
    "            text = str(text).lower()\n",
    "            # Remove special characters and extra whitespace\n",
    "            text = ' '.join(''.join(c if c.isalnum() else ' ' for c in text).split())\n",
    "            # Remove stop words and short tokens\n",
    "            tokens = [token for token in text.split() if token not in stop_words and len(token) > 2]\n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        return [clean_text(text) for text in texts]\n",
    "    \n",
    "    return preprocess_text\n",
    "\n",
    "# Set up NLTK and get the preprocessing function\n",
    "preprocess_text = setup_nltk_in_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T18:52:09.128930Z",
     "iopub.status.busy": "2024-10-26T18:52:09.127956Z",
     "iopub.status.idle": "2024-10-26T18:54:59.387022Z",
     "shell.execute_reply": "2024-10-26T18:54:59.385443Z",
     "shell.execute_reply.started": "2024-10-26T18:52:09.128876Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Topics:\n",
      "    Topic  Count                                        Name  \\\n",
      "0      -1  33232  -1_housing_singaporeans_income_singaporean   \n",
      "1       0   2345           0_drugs_singaporeans_crimes_crime   \n",
      "2       1    588           1_teachers_teacher_teaching_teach   \n",
      "3       2    557                  2_women_females_men_gender   \n",
      "4       3    547                 3_sauce_recipe_flavour_dish   \n",
      "5       4    500              4_weather_climate_rainy_winter   \n",
      "6       5    492           5_covid_pandemic_infection_immune   \n",
      "7       6    485                      6_jobs_job_hiring_hire   \n",
      "8       7    474                                       7____   \n",
      "9       8    440                    8_toilet_pee_drain_dirty   \n",
      "10      9    417       9_shipping_delivery_pricing_purchases   \n",
      "11     10    408              10_thanks_thank_welcome_cheers   \n",
      "12     11    255                        11_yes_yep_nope_yeah   \n",
      "13     12    253              12_landlord_rents_rent_renting   \n",
      "14     13    253                  13_sleep_sleeping_bed_wake   \n",
      "15     14    248         14_doctors_nurses_hospitals_medical   \n",
      "16     15    242        15_degree_graduates_graduate_careers   \n",
      "17     16    241                16_music_sound_noise_hearing   \n",
      "18     17    214              17_dating_single_singles_dates   \n",
      "19     18    208              18_scam_scams_scammed_scammers   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [housing, singaporeans, income, singaporean, s...   \n",
      "1   [drugs, singaporeans, crimes, crime, drug, sin...   \n",
      "2   [teachers, teacher, teaching, teach, students,...   \n",
      "3   [women, females, men, gender, males, woman, gi...   \n",
      "4   [sauce, recipe, flavour, dish, noodles, rice, ...   \n",
      "5   [weather, climate, rainy, winter, rain, temp, ...   \n",
      "6   [covid, pandemic, infection, immune, fever, di...   \n",
      "7   [jobs, job, hiring, hire, work, working, wage,...   \n",
      "8                                [, , , , , , , , , ]   \n",
      "9   [toilet, pee, drain, dirty, seat, bowl, clean,...   \n",
      "10  [shipping, delivery, pricing, purchases, selle...   \n",
      "11  [thanks, thank, welcome, cheers, glad, appreci...   \n",
      "12  [yes, yep, nope, yeah, absolutely, yah, yay, a...   \n",
      "13  [landlord, rents, rent, renting, rental, apart...   \n",
      "14  [sleep, sleeping, bed, wake, tired, woke, nigh...   \n",
      "15  [doctors, nurses, hospitals, medical, patients...   \n",
      "16  [degree, graduates, graduate, careers, grads, ...   \n",
      "17  [music, sound, noise, hearing, listening, ears...   \n",
      "18  [dating, single, singles, dates, date, relatio...   \n",
      "19  [scam, scams, scammed, scammers, spam, legitim...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [beamless flats smart crane systems piloted te...  \n",
      "1   [don really think deter drug lords more deter ...  \n",
      "2   [oyk overarching idea moe not want reduce clas...  \n",
      "3   [hahahahaha sorry got many successful wonderfu...  \n",
      "4   [where kambing sauce, chee cheong fun light so...  \n",
      "5   [uhh weather, take cali hawaii weather, people...  \n",
      "6                       [not covid yet, covid, covid]  \n",
      "7     [need job, nothing job now find job, found job]  \n",
      "8                                         [, , haven]  \n",
      "9   [mean one time office coworker turned said sme...  \n",
      "10  [qexpress delivery item unit price mean, just ...  \n",
      "11                           [thanks, thanks, thanks]  \n",
      "12                                    [yes, yes, yes]  \n",
      "13  [landlord salary, landlord why never change, l...  \n",
      "14            [sleep, 14hr sleep still sleepy, sleep]  \n",
      "15  [training local doctors like handing long term...  \n",
      "16  [depends whether degree dem juicy tech skills,...  \n",
      "17  [plug music, neighborhood unit always noisy pl...  \n",
      "18  [going dating app dates means dating, dating, ...  \n",
      "19                            [scam, scam, scam than]  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "yearmonth = '2022-10'\n",
    "df_sample = df[df['yearmonth'] == yearmonth]\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_texts = preprocess_text(df_sample['text'])\n",
    "\n",
    "# Initialize BERTopic model\n",
    "representation_model = KeyBERTInspired()\n",
    "vectorizer_model = CountVectorizer(min_df=10, stop_words=\"english\")\n",
    "topic_model = BERTopic(\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=\"auto\",\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probabilities = topic_model.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Get topic info\n",
    "print(\"\\nTop 20 Topics:\")\n",
    "print(topic_model.get_topic_info().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:26:10.856781Z",
     "iopub.status.busy": "2024-10-26T19:26:10.856371Z",
     "iopub.status.idle": "2024-10-26T19:58:22.485941Z",
     "shell.execute_reply": "2024-10-26T19:58:22.484774Z",
     "shell.execute_reply.started": "2024-10-26T19:26:10.856745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating overall coherence score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Coherence Score (C_v): 0.3890\n",
      "\n",
      "Calculating per-topic coherence scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f7561ae0550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-topic coherence scores:\n",
      "Topic 0: 0.5018\n",
      "Topic 1: 0.8172\n",
      "Topic 2: 0.7348\n",
      "Topic 3: 0.4741\n",
      "Topic 4: 0.6870\n",
      "Topic 5: 0.5177\n",
      "Topic 6: 0.5924\n",
      "Topic 8: 0.3151\n",
      "Topic 9: 0.2668\n",
      "Topic 10: 0.3469\n",
      "Topic 11: 0.3239\n",
      "Topic 12: 0.7671\n",
      "Topic 13: 0.5096\n",
      "Topic 14: 0.7815\n",
      "Topic 15: 0.5582\n",
      "Topic 16: 0.2685\n",
      "Topic 17: 0.6695\n",
      "Topic 18: 0.3281\n",
      "Topic 19: 0.6500\n",
      "Topic 20: 0.2392\n",
      "Topic 21: 0.6216\n",
      "Topic 22: 0.8706\n",
      "Topic 23: 0.3421\n",
      "Topic 24: 0.7085\n",
      "Topic 25: 0.3439\n",
      "Topic 26: 0.5631\n",
      "Topic 27: 0.3601\n",
      "Topic 28: 0.3036\n",
      "Topic 29: 0.5065\n",
      "Topic 30: 0.3955\n",
      "Topic 31: 0.4048\n",
      "Topic 32: 0.5611\n",
      "Topic 33: 0.6094\n",
      "Topic 34: 0.7270\n",
      "Topic 35: 0.2779\n",
      "Topic 36: 0.4091\n",
      "Topic 37: 0.3298\n",
      "Topic 38: 0.4045\n",
      "Topic 39: 0.2965\n",
      "Topic 40: 0.2907\n",
      "Topic 41: 0.2684\n",
      "Topic 42: 0.4786\n",
      "Topic 43: 0.3358\n",
      "Topic 44: 0.3479\n",
      "Topic 45: 0.3990\n",
      "Topic 46: 0.4045\n",
      "Topic 47: 0.6843\n",
      "Topic 48: 0.5755\n",
      "Topic 49: 0.3267\n",
      "Topic 50: 0.6768\n",
      "Topic 51: 0.4415\n",
      "Topic 52: 0.4582\n",
      "Topic 53: 0.3104\n",
      "Topic 54: 0.3015\n",
      "Topic 55: 0.3683\n",
      "Topic 56: 0.2832\n",
      "Topic 57: 0.3676\n",
      "Topic 58: 0.3526\n",
      "Topic 59: 0.6707\n",
      "Topic 60: 0.3614\n",
      "Topic 61: 0.4531\n",
      "Topic 62: 0.7480\n",
      "Topic 63: 0.3295\n",
      "Topic 64: 0.5900\n",
      "Topic 65: 0.5030\n",
      "Topic 66: 0.2762\n",
      "Topic 67: 0.3927\n",
      "Topic 68: 0.8059\n",
      "Topic 69: 0.4857\n",
      "Topic 70: 0.2913\n",
      "Topic 71: 0.3219\n",
      "Topic 72: 0.3186\n",
      "Topic 73: 0.4898\n",
      "Topic 74: 0.3787\n",
      "Topic 75: 0.3638\n",
      "Topic 76: 0.3481\n",
      "Topic 77: 0.3648\n",
      "Topic 78: 0.4108\n",
      "Topic 79: 0.3681\n",
      "Topic 80: 0.5284\n",
      "Topic 81: 0.3258\n",
      "Topic 82: 0.3237\n",
      "Topic 83: 0.8296\n",
      "Topic 84: 0.8642\n",
      "Topic 85: 0.2690\n",
      "Topic 86: 0.3858\n",
      "Topic 87: 0.4333\n",
      "Topic 88: 0.5099\n",
      "Topic 89: 0.3333\n",
      "Topic 90: 0.4591\n",
      "Topic 91: 0.3765\n",
      "Topic 92: 0.3804\n",
      "Topic 93: 0.4345\n",
      "Topic 94: 0.3329\n",
      "Topic 95: 0.3960\n",
      "Topic 96: 0.4185\n",
      "Topic 97: 0.3085\n",
      "Topic 98: 0.3062\n",
      "Topic 99: 0.3298\n",
      "Topic 100: 0.3793\n",
      "Topic 101: 0.3296\n",
      "Topic 102: 0.4098\n",
      "Topic 103: 0.4527\n",
      "Topic 104: 0.4100\n",
      "Topic 105: 0.6952\n",
      "Topic 106: 0.6682\n",
      "Topic 107: 0.3248\n",
      "Topic 108: 0.3615\n",
      "Topic 109: 0.7891\n",
      "Topic 110: 0.3213\n",
      "Topic 111: 0.4642\n",
      "Topic 112: 0.5538\n",
      "Topic 113: 0.3017\n",
      "Topic 114: 0.1983\n",
      "Topic 115: 0.5240\n",
      "Topic 116: 0.3758\n",
      "Topic 117: 0.4655\n",
      "Topic 118: 0.3196\n",
      "Topic 119: 0.2798\n",
      "Topic 120: 0.2798\n",
      "Topic 121: 0.5164\n",
      "Topic 122: 0.2624\n",
      "Topic 123: 0.3364\n",
      "Topic 124: 0.4006\n",
      "Topic 125: 0.2974\n",
      "Topic 126: 0.7464\n",
      "Topic 127: 0.3901\n",
      "Topic 128: 0.2547\n",
      "Topic 129: 0.3964\n",
      "Topic 130: 0.3458\n",
      "Topic 131: 0.2961\n",
      "Topic 132: 0.4502\n",
      "Topic 133: 0.4178\n",
      "Topic 134: 0.6011\n",
      "Topic 135: 0.5346\n",
      "Topic 136: 0.3438\n",
      "Topic 137: 0.2903\n",
      "Topic 138: 0.5589\n",
      "Topic 139: 0.2995\n",
      "Topic 140: 0.4224\n",
      "Topic 141: 0.2636\n",
      "Topic 142: 0.3885\n",
      "Topic 143: 0.3768\n",
      "Topic 144: 0.3103\n",
      "Topic 145: 0.4553\n",
      "Topic 146: 0.4911\n",
      "Topic 147: 0.3903\n",
      "Topic 148: 0.3112\n",
      "Topic 149: 0.3951\n",
      "Topic 150: 0.4270\n",
      "Topic 151: 0.4181\n",
      "Topic 152: 0.3833\n",
      "Topic 153: 0.2858\n",
      "Topic 154: 0.4721\n",
      "Topic 155: 0.3973\n",
      "Topic 156: 0.4344\n",
      "Topic 157: 0.6095\n",
      "Topic 158: 0.4008\n",
      "Topic 159: 0.2543\n",
      "Topic 160: 0.5614\n",
      "Topic 161: 0.3481\n",
      "Topic 162: 0.4186\n",
      "Topic 163: 0.6705\n",
      "Topic 164: 0.4300\n",
      "Topic 165: 0.3373\n",
      "Topic 166: 0.2752\n",
      "Topic 167: 0.3230\n",
      "Topic 168: 0.5446\n",
      "Topic 169: 0.3365\n",
      "Topic 170: 0.3792\n",
      "Topic 171: 0.2262\n",
      "Topic 172: 0.2183\n",
      "Topic 173: 0.3832\n",
      "Topic 174: 0.3093\n",
      "Topic 175: 0.2592\n",
      "Topic 176: 0.3581\n",
      "Topic 177: 0.3725\n",
      "Topic 178: 0.3450\n",
      "Topic 179: 0.4375\n",
      "Topic 180: 0.3119\n",
      "Topic 181: 0.1934\n",
      "Topic 182: 0.3383\n",
      "Topic 183: 0.5118\n",
      "Topic 184: 0.3197\n",
      "Topic 185: 0.4084\n",
      "Topic 186: 0.5669\n",
      "Topic 187: 0.5486\n",
      "Topic 188: 0.3230\n",
      "Topic 189: 0.3843\n",
      "Topic 190: 0.3110\n",
      "Topic 191: 0.3860\n",
      "Topic 192: 0.3726\n",
      "Topic 193: 0.4912\n",
      "Topic 194: 0.3566\n",
      "Topic 195: 0.4815\n",
      "Topic 196: 0.3561\n",
      "Topic 197: 0.4783\n",
      "Topic 198: 0.2888\n",
      "Topic 199: 0.4613\n",
      "Topic 200: 0.4214\n",
      "Topic 201: 0.3372\n",
      "Topic 202: 0.5243\n",
      "Topic 203: 0.3801\n",
      "Topic 204: 0.3365\n",
      "Topic 205: 0.3653\n",
      "Topic 206: 0.3365\n",
      "Topic 207: 0.2225\n",
      "Topic 208: 0.3612\n",
      "Topic 209: 0.3500\n",
      "Topic 210: 0.3814\n",
      "Topic 211: 0.3028\n",
      "Topic 212: 0.2633\n",
      "Topic 213: 0.5272\n",
      "Topic 214: 0.3585\n",
      "Topic 215: 0.4358\n",
      "Topic 216: 0.8429\n",
      "Topic 217: 0.2764\n",
      "Topic 218: 0.5104\n",
      "Topic 219: 0.3492\n",
      "Topic 220: 0.3211\n",
      "Topic 221: 0.3320\n",
      "Topic 222: 0.6476\n",
      "Topic 223: 0.3271\n",
      "Topic 224: 0.3391\n",
      "Topic 225: 0.3301\n",
      "Topic 226: 0.3285\n",
      "Topic 227: 0.2827\n",
      "Topic 228: 0.2909\n",
      "Topic 229: 0.3316\n",
      "Topic 230: 0.3300\n",
      "Topic 231: 0.2379\n",
      "Topic 232: 0.7433\n",
      "Topic 233: 0.2314\n",
      "Topic 234: 0.4863\n",
      "Topic 235: 0.3390\n",
      "Topic 236: 0.4087\n",
      "Topic 237: 0.2845\n",
      "Topic 238: 0.2701\n",
      "Topic 239: 0.3714\n",
      "Topic 240: 0.4155\n",
      "Topic 241: 0.3097\n",
      "Topic 242: 0.3481\n",
      "Topic 243: 0.3571\n",
      "Topic 244: 0.3277\n",
      "Topic 245: 0.4146\n",
      "Topic 246: 0.4459\n",
      "Topic 247: 0.3566\n",
      "Topic 248: 0.6296\n",
      "Topic 249: 0.3371\n",
      "Topic 250: 0.3308\n",
      "Topic 251: 0.6004\n",
      "Topic 252: 0.3337\n",
      "Topic 253: 0.2738\n",
      "Topic 254: 0.3447\n",
      "Topic 255: 0.2094\n",
      "Topic 256: 0.3236\n",
      "Topic 257: 0.7366\n",
      "Topic 258: 0.2821\n",
      "Topic 259: 0.3221\n",
      "Topic 260: 0.3020\n",
      "Topic 261: 0.3339\n",
      "Topic 262: 0.3521\n",
      "Topic 263: 0.3335\n",
      "Topic 264: 0.3780\n",
      "Topic 265: 0.5385\n",
      "Topic 266: 0.5532\n",
      "Topic 267: 0.3070\n",
      "Topic 268: 0.3815\n",
      "Topic 269: 0.4098\n",
      "Topic 270: 0.4205\n",
      "Topic 271: 0.4333\n",
      "Topic 272: 0.5829\n",
      "Topic 273: 0.3192\n",
      "Topic 274: 0.3942\n",
      "Topic 275: 0.4293\n",
      "Topic 276: 0.3051\n",
      "Topic 277: 0.3025\n",
      "Topic 278: 0.2753\n",
      "Topic 279: 0.3116\n",
      "Topic 280: 0.2533\n",
      "Topic 281: 0.4006\n",
      "Topic 282: 0.6938\n",
      "Topic 283: 0.4639\n",
      "Topic 284: 0.3024\n",
      "Topic 285: 0.5659\n",
      "Topic 286: 0.3159\n",
      "Topic 287: 0.4123\n",
      "Topic 288: 0.3844\n",
      "Topic 289: 0.3166\n",
      "Topic 290: 0.3891\n",
      "Topic 291: 0.4137\n",
      "Topic 292: 0.1708\n",
      "Topic 293: 0.4008\n",
      "Topic 294: 0.3742\n",
      "Topic 295: 0.5130\n",
      "Topic 296: 0.3313\n",
      "Topic 297: 0.3613\n",
      "Topic 298: 0.3910\n",
      "Topic 299: 0.3643\n",
      "Topic 300: 0.3362\n",
      "Topic 301: 0.3379\n",
      "Topic 302: 0.3582\n",
      "Topic 303: 0.6120\n",
      "Topic 304: 0.7185\n",
      "Topic 305: 0.2463\n",
      "Topic 306: 0.4809\n",
      "Topic 307: 0.3718\n",
      "Topic 308: 0.2841\n",
      "Topic 309: 0.3566\n",
      "Topic 310: 0.3117\n",
      "Topic 311: 0.4188\n",
      "Topic 312: 0.2660\n",
      "Topic 313: 0.4983\n",
      "Topic 314: 0.3306\n",
      "Topic 315: 0.3644\n",
      "Topic 316: 0.2820\n",
      "Topic 317: 0.2103\n",
      "Topic 318: 0.3968\n",
      "Topic 319: 0.4040\n",
      "Topic 320: 0.3053\n",
      "Topic 321: 0.2753\n",
      "Topic 322: 0.4942\n",
      "Topic 323: 0.3318\n",
      "Topic 324: 0.4816\n",
      "Topic 325: 0.3557\n",
      "Topic 326: 0.7011\n",
      "Topic 327: 0.3340\n",
      "Topic 328: 0.3608\n",
      "Topic 329: 0.4648\n",
      "Topic 330: 0.3164\n",
      "Topic 331: 0.3046\n",
      "Topic 332: 0.3095\n",
      "Topic 333: 0.4244\n",
      "Topic 334: 0.3134\n",
      "Topic 335: 0.4205\n",
      "Topic 336: 0.2929\n",
      "Topic 337: 0.2687\n",
      "Topic 338: 0.3975\n",
      "Topic 339: 0.4231\n",
      "Topic 340: 0.3634\n",
      "Topic 341: 0.3203\n",
      "Topic 342: 0.3030\n",
      "Topic 343: 0.4950\n",
      "Topic 344: 0.2899\n",
      "Topic 345: 0.2724\n",
      "Topic 346: 0.2628\n",
      "Topic 347: 0.2729\n",
      "Topic 348: 0.3510\n",
      "Topic 349: 0.3236\n",
      "Topic 350: 0.8550\n",
      "Topic 351: 0.3433\n",
      "Topic 352: 0.8402\n",
      "Topic 353: 0.3055\n",
      "Topic 354: 0.3798\n",
      "Topic 355: 0.3324\n",
      "Topic 356: 0.8053\n",
      "Topic 357: 0.3980\n",
      "Topic 358: 0.4659\n",
      "Topic 359: 0.3158\n",
      "Topic 360: 0.4331\n",
      "Topic 361: 0.4558\n",
      "Topic 362: 0.5209\n",
      "Topic 363: 0.3316\n",
      "Topic 364: 0.4738\n",
      "Topic 365: 0.2405\n",
      "Topic 366: 0.3261\n",
      "Topic 367: 0.2950\n",
      "Topic 368: 0.2451\n",
      "Topic 369: 0.3473\n",
      "Topic 370: 0.2648\n",
      "Topic 371: 0.4912\n",
      "Topic 372: 0.2165\n",
      "Topic 373: 0.3687\n",
      "Topic 374: 0.2958\n",
      "Topic 375: 0.2872\n",
      "Topic 376: 0.2797\n",
      "Topic 377: 0.4450\n",
      "Topic 378: 0.3612\n",
      "Topic 379: 0.5314\n",
      "Topic 380: 0.3429\n",
      "Topic 381: 0.3504\n",
      "Topic 382: 0.4275\n",
      "Topic 383: 0.2661\n",
      "Topic 384: 0.5138\n",
      "Topic 385: 0.4419\n",
      "Topic 386: 0.3979\n",
      "Topic 387: 0.2864\n",
      "Topic 388: 0.5960\n",
      "Topic 389: 0.5260\n",
      "Topic 390: 0.4002\n",
      "Topic 391: 0.2731\n",
      "Topic 392: 0.3753\n",
      "Topic 393: 0.0324\n",
      "Topic 394: 0.4497\n",
      "Topic 395: 0.2490\n",
      "Topic 396: 0.2544\n",
      "Topic 397: 0.2763\n",
      "Topic 398: 0.2905\n",
      "Topic 399: 0.4193\n",
      "Topic 400: 0.2680\n",
      "Topic 401: 0.2742\n",
      "Topic 402: 0.2988\n",
      "Topic 403: 0.3038\n",
      "Topic 404: 0.3753\n",
      "Topic 405: 1.0000\n",
      "Topic 406: 0.3452\n",
      "Topic 407: 0.2789\n",
      "Topic 408: 0.4478\n",
      "Topic 409: 0.9733\n",
      "Topic 410: 0.2806\n",
      "Topic 411: 0.4317\n",
      "Topic 412: 0.3063\n",
      "Topic 413: 0.3023\n",
      "Topic 414: 0.4278\n",
      "Topic 415: 0.3815\n",
      "Topic 416: 0.2721\n",
      "Topic 417: 0.3105\n",
      "Topic 418: 0.4341\n",
      "Topic 419: 0.3890\n",
      "Topic 421: 0.3698\n",
      "Topic 422: 0.3439\n",
      "Topic 423: 0.5032\n",
      "Topic 424: 0.5593\n",
      "Topic 425: 0.2628\n",
      "Topic 426: 0.3201\n",
      "Topic 427: 0.4509\n",
      "Topic 428: 0.3457\n",
      "Topic 429: 0.3687\n",
      "Topic 430: 0.2246\n",
      "Topic 431: 0.3904\n",
      "Topic 432: 0.5775\n",
      "Topic 433: 0.3481\n",
      "Topic 434: 0.2754\n",
      "Topic 435: 0.2530\n",
      "Topic 436: 0.4481\n",
      "Topic 437: 0.3608\n",
      "Topic 438: 0.3595\n",
      "Topic 439: 0.3268\n",
      "Topic 440: 0.3847\n",
      "Topic 441: 0.3088\n",
      "Topic 442: 0.3461\n",
      "Topic 443: 0.3373\n",
      "Topic 444: 0.6565\n",
      "Topic 445: 0.1664\n",
      "Topic 446: 0.2662\n",
      "Topic 447: 0.3320\n",
      "Topic 448: 0.2987\n",
      "Topic 449: 0.3381\n",
      "Topic 450: 0.3498\n",
      "Topic 451: 0.3138\n",
      "Topic 452: 0.3318\n",
      "Topic 453: 0.4204\n",
      "Topic 454: 0.3216\n",
      "Topic 455: 0.3803\n",
      "Topic 456: 0.3906\n",
      "Topic 457: 0.2813\n",
      "Topic 458: 0.2494\n",
      "Topic 459: 0.3620\n",
      "Topic 460: 0.4206\n",
      "Topic 461: 0.2717\n",
      "Topic 462: 0.2660\n",
      "Topic 463: 0.2215\n",
      "Topic 464: 0.4130\n",
      "Topic 465: 0.3827\n",
      "Topic 466: 0.2657\n",
      "Topic 467: 0.4497\n",
      "Topic 468: 0.3600\n",
      "Topic 469: 0.1768\n",
      "Topic 470: 0.3217\n",
      "Topic 471: 0.3976\n",
      "Topic 472: 0.3873\n",
      "Topic 473: 0.3224\n",
      "Topic 474: 0.7194\n",
      "Topic 475: 0.4193\n",
      "Topic 476: 0.3735\n",
      "Topic 477: 0.3594\n",
      "Topic 478: 0.3684\n",
      "Topic 479: 0.3335\n",
      "Topic 480: 0.4382\n",
      "Topic 481: 0.2126\n",
      "Topic 482: 0.5062\n",
      "Topic 483: 0.7042\n",
      "Topic 484: 0.5511\n",
      "Topic 485: 0.3131\n",
      "Topic 486: 0.6148\n",
      "Topic 487: 0.3630\n",
      "Topic 488: 0.4323\n",
      "Topic 489: 0.4237\n",
      "Topic 490: 0.3216\n",
      "Topic 491: 0.3577\n",
      "Topic 492: 0.2932\n",
      "Topic 493: 0.7348\n",
      "Topic 494: 0.2370\n",
      "Topic 495: 0.2731\n",
      "Topic 496: 0.3312\n",
      "Topic 497: 0.7284\n",
      "Topic 498: 0.1989\n",
      "Topic 499: 0.3716\n",
      "Topic 500: 0.2750\n",
      "Topic 501: 0.3924\n",
      "Topic 502: 0.4431\n",
      "Topic 503: 0.4201\n",
      "Topic 504: 1.0000\n",
      "Topic 505: 0.3052\n",
      "Topic 506: 0.3370\n",
      "Topic 507: 0.3002\n",
      "Topic 508: 0.2862\n",
      "Topic 509: 0.4921\n",
      "Topic 510: 0.3697\n",
      "Topic 511: 0.3821\n",
      "Topic 512: 0.2748\n",
      "Topic 513: 0.3345\n",
      "Topic 514: 0.2768\n",
      "Topic 515: 0.3320\n",
      "Topic 516: 0.6921\n",
      "Topic 517: 0.2710\n",
      "Topic 518: 0.2855\n",
      "Topic 519: 0.3060\n",
      "Topic 520: 0.4555\n",
      "Topic 521: 0.2943\n",
      "Topic 522: 0.2968\n",
      "Topic 523: 0.3201\n",
      "Topic 524: 0.2550\n",
      "Topic 525: 0.8589\n",
      "Topic 526: 0.4487\n",
      "Topic 527: 0.2437\n",
      "Topic 528: 0.2670\n",
      "Topic 529: 0.2197\n",
      "Topic 530: 0.3418\n",
      "Topic 531: 0.4612\n",
      "Topic 532: 0.2122\n",
      "Topic 533: 0.3950\n",
      "Topic 534: 0.3407\n",
      "Topic 535: 0.5040\n",
      "Topic 536: 0.7325\n",
      "Topic 537: 0.2642\n",
      "Topic 538: 0.3640\n",
      "Topic 539: 0.6258\n",
      "Topic 540: 0.2308\n",
      "Topic 541: 0.2890\n",
      "Topic 542: 0.4805\n",
      "Topic 543: 0.3480\n",
      "Topic 544: 0.3120\n",
      "Topic 545: 0.2370\n",
      "Topic 546: 0.4951\n",
      "Topic 547: 0.2772\n",
      "Topic 548: 0.5935\n",
      "Topic 549: 0.5949\n",
      "Topic 550: 0.3580\n",
      "Topic 551: 0.1892\n",
      "Topic 552: 0.3763\n",
      "Topic 553: 0.2687\n",
      "Topic 554: 0.4036\n",
      "Topic 555: 0.2877\n",
      "Topic 556: 0.1924\n",
      "Topic 557: 0.2806\n",
      "Topic 558: 0.4152\n",
      "Topic 559: 0.6527\n",
      "Topic 560: 0.5650\n",
      "Topic 561: 0.3820\n",
      "Topic 562: 0.3341\n",
      "Topic 563: 0.1841\n",
      "Topic 564: 0.2541\n",
      "Topic 565: 0.3145\n",
      "Topic 566: 0.3777\n",
      "Topic 567: 0.5007\n",
      "Topic 568: 0.3673\n",
      "Topic 569: 0.2044\n",
      "Topic 570: 0.3425\n",
      "Topic 571: 0.5110\n",
      "Topic 572: 0.2317\n",
      "Topic 573: 0.3319\n",
      "Topic 574: 0.3150\n",
      "Topic 575: 0.4375\n",
      "Topic 576: 0.6910\n",
      "Topic 577: 0.2588\n",
      "Topic 578: 0.2791\n",
      "Topic 579: 0.3641\n",
      "Topic 580: 0.4082\n",
      "Topic 581: 0.1473\n",
      "Topic 582: 0.7146\n",
      "Topic 583: 0.2757\n",
      "Topic 584: 0.3382\n",
      "Topic 585: 0.3408\n",
      "Topic 586: 0.4528\n",
      "Topic 587: 0.3521\n",
      "Topic 588: 0.3368\n",
      "Topic 589: 0.2772\n",
      "Topic 590: 0.3841\n",
      "Topic 591: 0.3020\n",
      "Topic 592: 0.2920\n",
      "Topic 593: 0.2965\n",
      "Topic 594: 0.3539\n",
      "Topic 595: 0.3839\n",
      "Topic 596: 0.3300\n",
      "Topic 597: 0.3187\n",
      "Topic 598: 0.2950\n",
      "Topic 599: 0.3382\n",
      "Topic 600: 0.3705\n",
      "Topic 601: 0.3646\n",
      "Topic 602: 0.2842\n",
      "Topic 603: 0.2771\n",
      "Topic 604: 0.3361\n",
      "Topic 605: 0.2741\n",
      "Topic 606: 0.2298\n",
      "Topic 607: 0.2267\n",
      "Topic 608: 1.0000\n",
      "Topic 609: 0.3986\n",
      "Topic 610: 0.3866\n",
      "Topic 611: 0.3663\n",
      "Topic 612: 0.3586\n",
      "Topic 613: 0.3865\n",
      "Topic 614: 0.4780\n",
      "Topic 615: 0.2867\n",
      "Topic 616: 0.2627\n",
      "Topic 617: 0.3290\n",
      "Topic 618: 0.2883\n",
      "Topic 619: 0.3000\n",
      "Topic 620: 0.2715\n",
      "Topic 621: 0.3346\n",
      "Topic 622: 0.5350\n",
      "Topic 623: 0.5895\n",
      "Topic 624: 0.6780\n",
      "Topic 625: 0.5837\n",
      "Topic 626: 0.6609\n",
      "Topic 627: 0.6161\n",
      "Topic 628: 0.3555\n",
      "Topic 629: 0.3549\n",
      "Topic 630: 0.3100\n",
      "Topic 631: 0.2789\n",
      "Topic 632: 0.2732\n",
      "Topic 633: 0.2645\n",
      "Topic 634: 0.2758\n",
      "Topic 635: 0.3399\n",
      "Topic 636: 0.2436\n",
      "Topic 637: 0.2908\n",
      "Topic 638: 0.3170\n",
      "Topic 639: 0.3958\n",
      "Topic 640: 0.1806\n",
      "Topic 641: 0.2282\n",
      "Topic 642: 0.3036\n",
      "Topic 643: 0.2500\n",
      "Topic 644: 0.4183\n",
      "Topic 645: 0.3430\n",
      "Topic 646: 0.3535\n",
      "Topic 647: 0.2779\n",
      "Topic 648: 0.2897\n",
      "Topic 649: 0.3078\n",
      "Topic 650: 0.4027\n",
      "Topic 651: 0.4214\n",
      "Topic 652: 0.3721\n",
      "Topic 653: 0.3450\n",
      "Topic 654: 0.2531\n",
      "Topic 655: 0.3813\n",
      "Topic 656: 0.3173\n",
      "Topic 657: 0.3508\n",
      "Topic 658: 0.2262\n",
      "Topic 659: 0.4377\n",
      "Topic 660: 0.3441\n",
      "Topic 661: 0.3461\n",
      "Topic 662: 0.1861\n",
      "Topic 663: 0.2559\n",
      "Topic 664: 0.3342\n",
      "Topic 665: 0.2563\n",
      "Topic 666: 0.3624\n",
      "Topic 667: 0.2262\n",
      "Topic 668: 0.4852\n",
      "Topic 669: 0.4304\n",
      "Topic 670: 0.3184\n",
      "Topic 671: 0.2685\n",
      "Topic 672: 0.5404\n",
      "Topic 673: 0.1810\n",
      "Topic 674: 0.4625\n",
      "Topic 675: 0.2508\n",
      "Topic 676: 0.4258\n",
      "Topic 677: 0.2984\n",
      "Topic 678: 0.3768\n",
      "Topic 679: 0.3525\n",
      "Topic 680: 0.2981\n",
      "Topic 681: 0.2450\n",
      "Topic 682: 0.2836\n",
      "Topic 683: 0.3928\n",
      "Topic 684: 0.3785\n",
      "Topic 685: 0.3753\n",
      "Topic 686: 0.3650\n",
      "Topic 687: 0.5407\n",
      "Topic 688: 0.3482\n",
      "Topic 689: 0.4962\n",
      "Topic 690: 0.2796\n",
      "Topic 691: 0.3690\n",
      "Topic 692: 0.3839\n",
      "Topic 693: 0.3229\n",
      "Topic 694: 0.3925\n",
      "Topic 695: 0.3364\n",
      "Topic 696: 0.5359\n",
      "Topic 697: 0.3186\n",
      "Topic 698: 0.5201\n",
      "Topic 699: 0.3871\n",
      "Topic 700: 0.3953\n",
      "Topic 701: 0.3704\n",
      "Topic 702: 0.3792\n",
      "Topic 703: 0.3748\n",
      "Topic 704: 0.2383\n",
      "Topic 705: 0.2974\n",
      "Topic 706: 0.2967\n",
      "Topic 707: 0.6171\n",
      "Topic 708: 0.2809\n",
      "Topic 709: 0.3465\n",
      "Topic 710: 0.3220\n",
      "Topic 711: 0.5903\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "def calculate_topic_coherence(topic_model, texts, topics):\n",
    "    \"\"\"\n",
    "    Calculate topic coherence for a BERTopic model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic_model : BERTopic model\n",
    "        The fitted BERTopic model\n",
    "    texts : list\n",
    "        List of preprocessed text documents\n",
    "    topics : list\n",
    "        List of assigned topics from fit_transform\n",
    "    \"\"\"\n",
    "    # Extract vectorizer and build tokenizer\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokens = [tokenizer(doc) for doc in texts]\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    \n",
    "    # Create mapping of word to id\n",
    "    word2id = {word: idx for idx, word in dictionary.items()}\n",
    "    \n",
    "    # Get topic words for each topic\n",
    "    unique_topics = sorted(set(topics) - {-1})  # Exclude -1 topic\n",
    "    topic_words = []\n",
    "    \n",
    "    for topic_idx in unique_topics:\n",
    "        # Get the words for this topic\n",
    "        topic = topic_model.get_topic(topic_idx)\n",
    "        if topic:  # Only include non-empty topics\n",
    "            # Extract just the words (not the weights)\n",
    "            words = [word for word, _ in topic[:10]]  # Take top 10 words per topic\n",
    "            # Convert words to dictionary ids\n",
    "            word_ids = [word2id[word] for word in words if word in word2id]\n",
    "            if word_ids:  # Only add if we have valid words\n",
    "                topic_words.append(word_ids)\n",
    "    \n",
    "    # Calculate coherence only if we have valid topics\n",
    "    if topic_words:\n",
    "        try:\n",
    "            coherence_model = CoherenceModel(\n",
    "                topics=topic_words,\n",
    "                texts=tokens,\n",
    "                dictionary=dictionary,\n",
    "                coherence='c_v'\n",
    "            )\n",
    "            return coherence_model.get_coherence()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating coherence: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def calculate_per_topic_coherence(topic_model, texts, topics):\n",
    "    \"\"\"\n",
    "    Calculate coherence scores for each individual topic\n",
    "    \"\"\"\n",
    "    # Extract vectorizer and build tokenizer\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokens = [tokenizer(doc) for doc in texts]\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    word2id = {word: idx for idx, word in dictionary.items()}\n",
    "    \n",
    "    unique_topics = sorted(set(topics) - {-1})  # Exclude -1 topic\n",
    "    per_topic_coherence = {}\n",
    "    \n",
    "    for topic_idx in unique_topics:\n",
    "        # Get the words for this topic\n",
    "        topic = topic_model.get_topic(topic_idx)\n",
    "        if topic:\n",
    "            # Extract just the words (not the weights)\n",
    "            words = [word for word, _ in topic[:10]]  # Take top 10 words\n",
    "            # Convert words to dictionary ids\n",
    "            word_ids = [word2id[word] for word in words if word in word2id]\n",
    "            if word_ids:\n",
    "                try:\n",
    "                    coherence_model = CoherenceModel(\n",
    "                        topics=[word_ids],\n",
    "                        texts=tokens,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v'\n",
    "                    )\n",
    "                    per_topic_coherence[topic_idx] = coherence_model.get_coherence()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating coherence for topic {topic_idx}: {e}\")\n",
    "                    per_topic_coherence[topic_idx] = None\n",
    "    \n",
    "    return per_topic_coherence\n",
    "\n",
    "# Calculate coherence score\n",
    "print(\"Calculating overall coherence score...\")\n",
    "coherence_score = calculate_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "\n",
    "if coherence_score is not None:\n",
    "    print(f\"\\nTopic Coherence Score (C_v): {coherence_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate overall coherence score\")\n",
    "\n",
    "# Calculate and print per-topic coherence scores\n",
    "print(\"\\nCalculating per-topic coherence scores...\")\n",
    "per_topic_scores = calculate_per_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "print(\"\\nPer-topic coherence scores:\")\n",
    "for topic, score in per_topic_scores.items():\n",
    "    if score is not None:\n",
    "        print(f\"Topic {topic}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"Topic {topic}: Could not calculate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T20:37:17.588668Z",
     "iopub.status.busy": "2024-10-26T20:37:17.588221Z",
     "iopub.status.idle": "2024-10-26T20:41:19.365298Z",
     "shell.execute_reply": "2024-10-26T20:41:19.364028Z",
     "shell.execute_reply.started": "2024-10-26T20:37:17.588628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Topics:\n",
      "    Topic  Count                                               Name  \\\n",
      "0      -1  44869   -1_singaporeans_singaporean_government_singapore   \n",
      "1       0   1285  0_singaporeans_singaporean_singapore_singaporeraw   \n",
      "2       1   1082                    1_driving_accidents_roads_drove   \n",
      "3       2    854                                              2____   \n",
      "4       3    676                     3_thank_thanks_welcome_amazing   \n",
      "5       4    635                      4_covid_vaccines_pandemic_flu   \n",
      "6       5    606                    5_equality_genders_gender_equal   \n",
      "7       6    543                 6_crypto_gambling_gamble_investing   \n",
      "8       7    430                   7_content_messaging_message_blog   \n",
      "9       8    424                  8_parents_parent_parenting_family   \n",
      "10      9    371                   9_chicken_frozen_freeze_freezing   \n",
      "11     10    368            10_depression_mentally_mental_illnesses   \n",
      "12     11    358                         11_consent_rape_laws_drunk   \n",
      "13     12    353                         12_scam_spam_legit_genuine   \n",
      "14     13    321                 13_teachers_teacher_teaching_teach   \n",
      "15     14    314                        14_pants_wear_shorts_attire   \n",
      "16     15    311                        15_smell_smelly_sour_breath   \n",
      "17     16    311                 16_muslim_religion_islam_religious   \n",
      "18     17    309                        17_lmao_lol_hahaha_hahahaha   \n",
      "19     18    303                    18_sleep_sleeping_asleep_waking   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [singaporeans, singaporean, government, singap...   \n",
      "1   [singaporeans, singaporean, singapore, singapo...   \n",
      "2   [driving, accidents, roads, drove, dogs, accid...   \n",
      "3                                [, , , , , , , , , ]   \n",
      "4   [thank, thanks, welcome, amazing, appreciate, ...   \n",
      "5   [covid, vaccines, pandemic, flu, infection, vi...   \n",
      "6   [equality, genders, gender, equal, females, wo...   \n",
      "7   [crypto, gambling, gamble, investing, invest, ...   \n",
      "8   [content, messaging, message, blog, posts, lin...   \n",
      "9   [parents, parent, parenting, family, siblings,...   \n",
      "10  [chicken, frozen, freeze, freezing, meat, beef...   \n",
      "11  [depression, mentally, mental, illnesses, heal...   \n",
      "12  [consent, rape, laws, drunk, law, courts, beha...   \n",
      "13  [scam, spam, legit, genuine, seller, confirmed...   \n",
      "14  [teachers, teacher, teaching, teach, students,...   \n",
      "15  [pants, wear, shorts, attire, dress, clothing,...   \n",
      "16  [smell, smelly, sour, breath, sweating, wash, ...   \n",
      "17  [muslim, religion, islam, religious, muslims, ...   \n",
      "18  [lmao, lol, hahaha, hahahaha, hah, haha, hahah...   \n",
      "19  [sleep, sleeping, asleep, waking, awake, wake,...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [accept process screening entry generally trus...  \n",
      "1   [must china, just like singaporean namesake, w...  \n",
      "2   [watched video dog run see dog owner making so...  \n",
      "3                                              [, , ]  \n",
      "4                               [thank, thank, thank]  \n",
      "5   [covid anyone ddd, sneaky covid more variants ...  \n",
      "6   [first day gender equality, should women natio...  \n",
      "7   [anyone know where good source read crypto, ca...  \n",
      "8   [view link https redditsave com info url singa...  \n",
      "9   [both good parents, parents monsters, all pare...  \n",
      "10  [time flexitarian frozen chicken never juicy, ...  \n",
      "11  [don know mental health issues also, hi5 menta...  \n",
      "12  [let leave fact girl also initiated kissing co...  \n",
      "13                    [scam, lust scam too, how scam]  \n",
      "14  [forum give teachers authority remove disrespe...  \n",
      "15  [men wear usually city area going date there t...  \n",
      "16  [want fuckboy smell, also cannot smell, can on...  \n",
      "17  [should start religion, better religion, musli...  \n",
      "18                                 [lmao, lmao, lmao]  \n",
      "19                    [sleep, can sleep, sleep prius]  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "yearmonth = '2022-05'\n",
    "df_sample = df[df['yearmonth'] == yearmonth]\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_texts = preprocess_text(df_sample['text'])\n",
    "\n",
    "# Initialize BERTopic model\n",
    "representation_model = KeyBERTInspired()\n",
    "vectorizer_model = CountVectorizer(min_df=10, stop_words=\"english\")\n",
    "topic_model = BERTopic(\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=\"auto\",\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probabilities = topic_model.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Get topic info\n",
    "print(\"\\nTop 20 Topics:\")\n",
    "print(topic_model.get_topic_info().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T20:43:17.554972Z",
     "iopub.status.busy": "2024-10-26T20:43:17.554523Z",
     "iopub.status.idle": "2024-10-26T21:47:04.959164Z",
     "shell.execute_reply": "2024-10-26T21:47:04.957972Z",
     "shell.execute_reply.started": "2024-10-26T20:43:17.554911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating overall coherence score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Coherence Score (C_v): 0.3901\n",
      "\n",
      "Calculating per-topic coherence scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f7561ae0550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-topic coherence scores:\n",
      "Topic 0: 0.5769\n",
      "Topic 1: 0.6708\n",
      "Topic 3: 0.3249\n",
      "Topic 4: 0.6073\n",
      "Topic 5: 0.7821\n",
      "Topic 6: 0.4785\n",
      "Topic 7: 0.4510\n",
      "Topic 8: 0.4250\n",
      "Topic 9: 0.5488\n",
      "Topic 10: 0.5001\n",
      "Topic 11: 0.5086\n",
      "Topic 12: 0.3564\n",
      "Topic 13: 0.8193\n",
      "Topic 14: 0.5896\n",
      "Topic 15: 0.3230\n",
      "Topic 16: 0.8911\n",
      "Topic 17: 0.3441\n",
      "Topic 18: 0.5800\n",
      "Topic 19: 0.4380\n",
      "Topic 20: 0.4151\n",
      "Topic 21: 0.2726\n",
      "Topic 22: 0.3690\n",
      "Topic 23: 0.3916\n",
      "Topic 24: 0.4173\n",
      "Topic 25: 0.5947\n",
      "Topic 26: 0.4717\n",
      "Topic 27: 0.5592\n",
      "Topic 28: 0.7513\n",
      "Topic 29: 0.4695\n",
      "Topic 30: 0.5867\n",
      "Topic 31: 0.3973\n",
      "Topic 32: 0.4046\n",
      "Topic 33: 0.7872\n",
      "Topic 34: 0.6396\n",
      "Topic 35: 0.3281\n",
      "Topic 36: 0.6572\n",
      "Topic 37: 0.3398\n",
      "Topic 38: 0.4067\n",
      "Topic 39: 0.5303\n",
      "Topic 40: 0.3964\n",
      "Topic 41: 0.3653\n",
      "Topic 42: 0.3214\n",
      "Topic 43: 0.5773\n",
      "Topic 44: 0.4333\n",
      "Topic 45: 0.3515\n",
      "Topic 46: 0.3013\n",
      "Topic 47: 0.4712\n",
      "Topic 48: 0.4547\n",
      "Topic 49: 0.3378\n",
      "Topic 50: 0.4841\n",
      "Topic 51: 0.5235\n",
      "Topic 52: 0.4391\n",
      "Topic 53: 0.6465\n",
      "Topic 54: 0.4815\n",
      "Topic 55: 0.3963\n",
      "Topic 56: 0.4083\n",
      "Topic 57: 0.3488\n",
      "Topic 58: 0.5060\n",
      "Topic 59: 0.3304\n",
      "Topic 60: 0.3647\n",
      "Topic 61: 0.3547\n",
      "Topic 62: 0.3344\n",
      "Topic 63: 0.5466\n",
      "Topic 64: 0.3315\n",
      "Topic 65: 0.3590\n",
      "Topic 66: 0.3702\n",
      "Topic 67: 0.4321\n",
      "Topic 68: 0.3533\n",
      "Topic 69: 0.3289\n",
      "Topic 70: 0.3448\n",
      "Topic 71: 0.4967\n",
      "Topic 72: 0.1695\n",
      "Topic 73: 0.4124\n",
      "Topic 74: 0.5713\n",
      "Topic 75: 0.4583\n",
      "Topic 76: 0.5070\n",
      "Topic 77: 0.3785\n",
      "Topic 78: 0.3702\n",
      "Topic 79: 0.3347\n",
      "Topic 80: 0.4247\n",
      "Topic 81: 0.4168\n",
      "Topic 82: 0.7304\n",
      "Topic 83: 0.6267\n",
      "Topic 84: 0.4430\n",
      "Topic 85: 0.3293\n",
      "Topic 86: 0.3999\n",
      "Topic 87: 0.3991\n",
      "Topic 88: 0.3751\n",
      "Topic 89: 0.7372\n",
      "Topic 90: 0.3014\n",
      "Topic 91: 0.3418\n",
      "Topic 92: 0.2278\n",
      "Topic 93: 0.7579\n",
      "Topic 94: 0.3483\n",
      "Topic 95: 0.3245\n",
      "Topic 96: 0.2989\n",
      "Topic 97: 0.3798\n",
      "Topic 98: 0.6747\n",
      "Topic 99: 0.7364\n",
      "Topic 100: 0.3582\n",
      "Topic 101: 0.3293\n",
      "Topic 102: 0.5452\n",
      "Topic 103: 0.7206\n",
      "Topic 104: 0.2653\n",
      "Topic 105: 0.2955\n",
      "Topic 106: 0.4237\n",
      "Topic 107: 0.5601\n",
      "Topic 108: 0.3677\n",
      "Topic 109: 0.2734\n",
      "Topic 110: 0.3552\n",
      "Topic 111: 0.4036\n",
      "Topic 112: 0.7056\n",
      "Topic 113: 0.5665\n",
      "Topic 114: 0.7270\n",
      "Topic 115: 0.3752\n",
      "Topic 116: 0.3672\n",
      "Topic 117: 0.5701\n",
      "Topic 118: 0.3584\n",
      "Topic 119: 0.6248\n",
      "Topic 120: 0.6516\n",
      "Topic 121: 0.4307\n",
      "Topic 122: 0.3263\n",
      "Topic 123: 0.5159\n",
      "Topic 124: 0.3655\n",
      "Topic 125: 0.3409\n",
      "Topic 126: 0.6266\n",
      "Topic 127: 0.6479\n",
      "Topic 128: 0.3633\n",
      "Topic 129: 0.3482\n",
      "Topic 130: 0.3337\n",
      "Topic 131: 0.3563\n",
      "Topic 132: 0.3851\n",
      "Topic 133: 0.1997\n",
      "Topic 134: 0.2955\n",
      "Topic 135: 0.4334\n",
      "Topic 136: 0.5758\n",
      "Topic 137: 0.3538\n",
      "Topic 138: 0.4469\n",
      "Topic 139: 0.5062\n",
      "Topic 140: 0.2628\n",
      "Topic 141: 0.3869\n",
      "Topic 142: 0.3817\n",
      "Topic 143: 0.3240\n",
      "Topic 144: 0.3504\n",
      "Topic 145: 0.3426\n",
      "Topic 146: 0.5083\n",
      "Topic 147: 0.3934\n",
      "Topic 148: 0.2826\n",
      "Topic 149: 0.6775\n",
      "Topic 150: 0.3309\n",
      "Topic 151: 0.4291\n",
      "Topic 152: 0.2513\n",
      "Topic 153: 0.3457\n",
      "Topic 154: 0.2855\n",
      "Topic 155: 0.4022\n",
      "Topic 156: 0.3567\n",
      "Topic 157: 0.3533\n",
      "Topic 158: 0.3365\n",
      "Topic 159: 0.3864\n",
      "Topic 160: 0.4770\n",
      "Topic 161: 0.4407\n",
      "Topic 162: 0.3370\n",
      "Topic 163: 0.4867\n",
      "Topic 164: 0.3416\n",
      "Topic 165: 0.3538\n",
      "Topic 166: 0.2309\n",
      "Topic 167: 0.6782\n",
      "Topic 168: 0.3039\n",
      "Topic 169: 0.3456\n",
      "Topic 170: 0.3645\n",
      "Topic 171: 0.3954\n",
      "Topic 172: 0.4474\n",
      "Topic 173: 0.4529\n",
      "Topic 174: 0.5410\n",
      "Topic 175: 0.5974\n",
      "Topic 176: 0.3140\n",
      "Topic 177: 0.3841\n",
      "Topic 178: 0.3047\n",
      "Topic 179: 0.4944\n",
      "Topic 180: 0.4248\n",
      "Topic 181: 0.4340\n",
      "Topic 182: 0.3787\n",
      "Topic 183: 0.7287\n",
      "Topic 184: 0.4010\n",
      "Topic 185: 0.4021\n",
      "Topic 186: 0.3419\n",
      "Topic 187: 0.2980\n",
      "Topic 188: 0.4216\n",
      "Topic 189: 0.3680\n",
      "Topic 190: 0.3906\n",
      "Topic 191: 0.3644\n",
      "Topic 192: 0.3142\n",
      "Topic 193: 0.3468\n",
      "Topic 194: 0.4779\n",
      "Topic 195: 0.4466\n",
      "Topic 196: 0.5245\n",
      "Topic 197: 0.2857\n",
      "Topic 198: 0.3909\n",
      "Topic 199: 0.3835\n",
      "Topic 200: 0.7841\n",
      "Topic 201: 0.3171\n",
      "Topic 202: 0.3929\n",
      "Topic 203: 0.5057\n",
      "Topic 204: 0.4392\n",
      "Topic 205: 0.5317\n",
      "Topic 206: 0.5290\n",
      "Topic 207: 0.3451\n",
      "Topic 208: 0.2848\n",
      "Topic 209: 0.3961\n",
      "Topic 210: 0.4498\n",
      "Topic 211: 0.4198\n",
      "Topic 212: 0.2594\n",
      "Topic 213: 0.2254\n",
      "Topic 214: 0.2753\n",
      "Topic 215: 0.3397\n",
      "Topic 216: 0.5344\n",
      "Topic 217: 0.3064\n",
      "Topic 218: 0.3400\n",
      "Topic 219: 0.5534\n",
      "Topic 220: 0.3988\n",
      "Topic 221: 0.8436\n",
      "Topic 222: 0.3468\n",
      "Topic 223: 0.6725\n",
      "Topic 224: 0.2970\n",
      "Topic 225: 0.8517\n",
      "Topic 226: 0.3648\n",
      "Topic 227: 0.7108\n",
      "Topic 228: 0.3699\n",
      "Topic 229: 0.2657\n",
      "Topic 230: 0.3528\n",
      "Topic 231: 0.3344\n",
      "Topic 232: 0.3591\n",
      "Topic 233: 0.2517\n",
      "Topic 234: 0.2930\n",
      "Topic 235: 0.3956\n",
      "Topic 236: 0.2955\n",
      "Topic 237: 0.3938\n",
      "Topic 238: 0.3244\n",
      "Topic 239: 0.4569\n",
      "Topic 240: 0.3054\n",
      "Topic 241: 0.3085\n",
      "Topic 242: 0.4606\n",
      "Topic 243: 0.4106\n",
      "Topic 244: 0.6410\n",
      "Topic 245: 0.4618\n",
      "Topic 246: 0.3158\n",
      "Topic 247: 0.4026\n",
      "Topic 248: 0.3808\n",
      "Topic 249: 0.3009\n",
      "Topic 250: 0.6609\n",
      "Topic 251: 0.3224\n",
      "Topic 252: 0.5756\n",
      "Topic 253: 0.3082\n",
      "Topic 254: 0.4238\n",
      "Topic 255: 0.3836\n",
      "Topic 256: 0.3748\n",
      "Topic 257: 0.4183\n",
      "Topic 258: 0.4004\n",
      "Topic 259: 0.4138\n",
      "Topic 260: 0.4633\n",
      "Topic 261: 0.3040\n",
      "Topic 262: 0.5528\n",
      "Topic 263: 0.4356\n",
      "Topic 264: 0.3044\n",
      "Topic 265: 0.2787\n",
      "Topic 266: 0.7238\n",
      "Topic 267: 0.2317\n",
      "Topic 268: 0.6324\n",
      "Topic 269: 0.2610\n",
      "Topic 270: 0.3551\n",
      "Topic 271: 0.4122\n",
      "Topic 272: 0.6066\n",
      "Topic 273: 0.3990\n",
      "Topic 274: 0.3882\n",
      "Topic 275: 0.5887\n",
      "Topic 276: 0.6079\n",
      "Topic 277: 0.2992\n",
      "Topic 278: 0.3729\n",
      "Topic 279: 0.5608\n",
      "Topic 280: 0.2072\n",
      "Topic 281: 0.3930\n",
      "Topic 282: 0.4168\n",
      "Topic 283: 0.4037\n",
      "Topic 284: 0.3135\n",
      "Topic 285: 0.3890\n",
      "Topic 286: 0.2957\n",
      "Topic 287: 0.2675\n",
      "Topic 288: 0.4685\n",
      "Topic 289: 0.4054\n",
      "Topic 290: 0.3280\n",
      "Topic 291: 0.2825\n",
      "Topic 292: 0.3326\n",
      "Topic 293: 0.3179\n",
      "Topic 294: 0.3136\n",
      "Topic 295: 0.3324\n",
      "Topic 296: 0.3145\n",
      "Topic 297: 0.4533\n",
      "Topic 298: 0.4082\n",
      "Topic 299: 0.3300\n",
      "Topic 300: 0.3486\n",
      "Topic 301: 0.3396\n",
      "Topic 302: 0.4545\n",
      "Topic 303: 0.3288\n",
      "Topic 304: 0.3243\n",
      "Topic 305: 0.2982\n",
      "Topic 306: 0.4727\n",
      "Topic 307: 0.2864\n",
      "Topic 308: 0.4829\n",
      "Topic 309: 0.6650\n",
      "Topic 310: 0.3091\n",
      "Topic 311: 0.3527\n",
      "Topic 312: 0.3591\n",
      "Topic 313: 0.2822\n",
      "Topic 314: 0.4138\n",
      "Topic 315: 0.3925\n",
      "Topic 316: 0.3605\n",
      "Topic 317: 0.2684\n",
      "Topic 318: 0.5413\n",
      "Topic 319: 0.2584\n",
      "Topic 320: 0.4503\n",
      "Topic 321: 0.2950\n",
      "Topic 322: 0.2421\n",
      "Topic 323: 0.4305\n",
      "Topic 324: 0.3479\n",
      "Topic 325: 0.7122\n",
      "Topic 326: 0.4933\n",
      "Topic 327: 0.3721\n",
      "Topic 328: 0.3489\n",
      "Topic 329: 0.3312\n",
      "Topic 330: 0.3255\n",
      "Topic 331: 0.2398\n",
      "Topic 332: 0.2476\n",
      "Topic 333: 0.5816\n",
      "Topic 334: 0.3372\n",
      "Topic 335: 0.4095\n",
      "Topic 336: 0.4272\n",
      "Topic 337: 0.3830\n",
      "Topic 338: 0.3634\n",
      "Topic 339: 0.3525\n",
      "Topic 340: 0.4625\n",
      "Topic 341: 0.4936\n",
      "Topic 342: 0.0498\n",
      "Topic 343: 0.3837\n",
      "Topic 344: 0.4142\n",
      "Topic 345: 0.4120\n",
      "Topic 346: 0.2415\n",
      "Topic 347: 0.6780\n",
      "Topic 348: 0.5159\n",
      "Topic 349: 0.3316\n",
      "Topic 350: 0.3695\n",
      "Topic 351: 0.2579\n",
      "Topic 352: 0.2998\n",
      "Topic 353: 0.5134\n",
      "Topic 354: 0.3402\n",
      "Topic 355: 0.3929\n",
      "Topic 356: 0.3226\n",
      "Topic 357: 0.5099\n",
      "Topic 358: 0.4742\n",
      "Topic 359: 0.3096\n",
      "Topic 360: 0.3339\n",
      "Topic 361: 0.2547\n",
      "Topic 362: 0.3201\n",
      "Topic 363: 0.3232\n",
      "Topic 364: 0.3588\n",
      "Topic 365: 0.3480\n",
      "Topic 366: 0.3132\n",
      "Topic 367: 0.3015\n",
      "Topic 368: 0.3696\n",
      "Topic 369: 0.4427\n",
      "Topic 370: 0.5570\n",
      "Topic 371: 0.2734\n",
      "Topic 372: 0.2808\n",
      "Topic 373: 0.3257\n",
      "Topic 374: 0.3911\n",
      "Topic 375: 0.3490\n",
      "Topic 376: 0.2535\n",
      "Topic 377: 0.3429\n",
      "Topic 378: 0.2887\n",
      "Topic 379: 0.4039\n",
      "Topic 380: 0.3086\n",
      "Topic 381: 0.3045\n",
      "Topic 382: 0.3171\n",
      "Topic 383: 0.5938\n",
      "Topic 384: 0.2756\n",
      "Topic 385: 0.3510\n",
      "Topic 386: 0.3223\n",
      "Topic 387: 0.3742\n",
      "Topic 388: 0.2642\n",
      "Topic 389: 0.2780\n",
      "Topic 390: 0.3251\n",
      "Topic 391: 0.5081\n",
      "Topic 392: 0.2655\n",
      "Topic 393: 0.3564\n",
      "Topic 394: 0.3105\n",
      "Topic 395: 0.2830\n",
      "Topic 396: 0.3153\n",
      "Topic 397: 0.2837\n",
      "Topic 398: 0.3865\n",
      "Topic 399: 0.4881\n",
      "Topic 400: 0.3541\n",
      "Topic 401: 0.3213\n",
      "Topic 402: 0.5478\n",
      "Topic 403: 0.3151\n",
      "Topic 404: 0.2581\n",
      "Topic 405: 0.3302\n",
      "Topic 406: 0.7502\n",
      "Topic 407: 0.4690\n",
      "Topic 408: 0.3008\n",
      "Topic 409: 0.2809\n",
      "Topic 410: 0.3102\n",
      "Topic 411: 0.3454\n",
      "Topic 412: 0.3704\n",
      "Topic 413: 0.3140\n",
      "Topic 414: 0.3650\n",
      "Topic 415: 0.4645\n",
      "Topic 416: 0.3845\n",
      "Topic 417: 0.3748\n",
      "Topic 418: 0.3784\n",
      "Topic 419: 0.3221\n",
      "Topic 420: 0.2433\n",
      "Topic 421: 0.3643\n",
      "Topic 422: 0.3764\n",
      "Topic 423: 0.2294\n",
      "Topic 424: 0.3230\n",
      "Topic 425: 0.2352\n",
      "Topic 426: 0.3439\n",
      "Topic 427: 0.1777\n",
      "Topic 428: 0.6364\n",
      "Topic 429: 0.2115\n",
      "Topic 430: 0.2528\n",
      "Topic 431: 0.2761\n",
      "Topic 432: 0.3841\n",
      "Topic 433: 0.2516\n",
      "Topic 434: 0.4594\n",
      "Topic 435: 0.2634\n",
      "Topic 436: 0.3540\n",
      "Topic 437: 0.3078\n",
      "Topic 438: 0.3299\n",
      "Topic 439: 0.5410\n",
      "Topic 440: 0.2591\n",
      "Topic 441: 0.3201\n",
      "Topic 442: 0.3462\n",
      "Topic 443: 0.3454\n",
      "Topic 444: 0.4717\n",
      "Topic 445: 0.3794\n",
      "Topic 446: 0.2769\n",
      "Topic 447: 0.2711\n",
      "Topic 448: 0.3840\n",
      "Topic 449: 0.5948\n",
      "Topic 450: 0.3873\n",
      "Topic 451: 0.3615\n",
      "Topic 452: 0.3337\n",
      "Topic 453: 0.3230\n",
      "Topic 454: 0.3917\n",
      "Topic 455: 0.3282\n",
      "Topic 456: 0.3977\n",
      "Topic 457: 0.3396\n",
      "Topic 458: 0.4483\n",
      "Topic 459: 0.4156\n",
      "Topic 460: 0.2473\n",
      "Topic 461: 0.3699\n",
      "Topic 462: 0.2540\n",
      "Topic 463: 0.2865\n",
      "Topic 464: 0.3417\n",
      "Topic 465: 0.3073\n",
      "Topic 466: 0.4628\n",
      "Topic 467: 0.2632\n",
      "Topic 468: 0.2702\n",
      "Topic 469: 0.2748\n",
      "Topic 470: 0.4231\n",
      "Topic 471: 0.3364\n",
      "Topic 472: 0.2415\n",
      "Topic 473: 0.3552\n",
      "Topic 474: 0.3732\n",
      "Topic 475: 0.2506\n",
      "Topic 476: 0.2280\n",
      "Topic 477: 0.3778\n",
      "Topic 478: 0.4768\n",
      "Topic 479: 0.3848\n",
      "Topic 480: 0.2536\n",
      "Topic 481: 0.3173\n",
      "Topic 482: 0.2889\n",
      "Topic 483: 0.3827\n",
      "Topic 484: 0.3446\n",
      "Topic 485: 0.3293\n",
      "Topic 486: 1.0000\n",
      "Topic 487: 0.4164\n",
      "Topic 488: 0.2479\n",
      "Topic 489: 0.2753\n",
      "Topic 490: 0.4569\n",
      "Topic 491: 0.4547\n",
      "Topic 492: 0.3446\n",
      "Topic 493: 0.3365\n",
      "Topic 494: 0.3661\n",
      "Topic 495: 0.3436\n",
      "Topic 496: 0.4501\n",
      "Topic 497: 0.2875\n",
      "Topic 498: 0.7414\n",
      "Topic 499: 0.3197\n",
      "Topic 500: 0.3604\n",
      "Topic 501: 0.4361\n",
      "Topic 502: 0.3108\n",
      "Topic 503: 0.5967\n",
      "Topic 504: 0.5063\n",
      "Topic 505: 0.3551\n",
      "Topic 506: 0.4212\n",
      "Topic 507: 0.3603\n",
      "Topic 508: 0.3131\n",
      "Topic 509: 0.3861\n",
      "Topic 510: 0.4190\n",
      "Topic 511: 0.3703\n",
      "Topic 512: 0.3222\n",
      "Topic 513: 0.5483\n",
      "Topic 514: 0.5452\n",
      "Topic 515: 0.4876\n",
      "Topic 516: 0.3632\n",
      "Topic 517: 0.2625\n",
      "Topic 518: 0.2788\n",
      "Topic 519: 0.4005\n",
      "Topic 520: 0.2974\n",
      "Topic 521: 0.5995\n",
      "Topic 522: 0.3916\n",
      "Topic 523: 0.2166\n",
      "Topic 524: 0.4119\n",
      "Topic 525: 0.6156\n",
      "Topic 526: 0.3568\n",
      "Topic 527: 0.4165\n",
      "Topic 528: 0.3739\n",
      "Topic 529: 0.4283\n",
      "Topic 530: 0.3009\n",
      "Topic 531: 0.2977\n",
      "Topic 532: 0.4398\n",
      "Topic 533: 0.2622\n",
      "Topic 534: 0.3261\n",
      "Topic 535: 0.3221\n",
      "Topic 536: 0.2974\n",
      "Topic 537: 0.3585\n",
      "Topic 538: 0.2993\n",
      "Topic 539: 0.3568\n",
      "Topic 540: 0.4276\n",
      "Topic 541: 0.2821\n",
      "Topic 542: 0.3543\n",
      "Topic 543: 0.4280\n",
      "Topic 544: 0.3331\n",
      "Topic 545: 0.2524\n",
      "Topic 546: 0.3287\n",
      "Topic 547: 0.3997\n",
      "Topic 548: 0.3976\n",
      "Topic 549: 0.3302\n",
      "Topic 550: 0.3131\n",
      "Topic 551: 0.7758\n",
      "Topic 552: 0.6444\n",
      "Topic 553: 0.2932\n",
      "Topic 554: 0.3762\n",
      "Topic 555: 0.2524\n",
      "Topic 556: 0.5061\n",
      "Topic 557: 0.3483\n",
      "Topic 558: 0.3678\n",
      "Topic 559: 0.3264\n",
      "Topic 560: 0.3573\n",
      "Topic 561: 0.2571\n",
      "Topic 562: 0.5853\n",
      "Topic 563: 0.3078\n",
      "Topic 564: 0.5301\n",
      "Topic 565: 0.4320\n",
      "Topic 566: 0.3255\n",
      "Topic 567: 0.3031\n",
      "Topic 568: 0.2945\n",
      "Topic 569: 0.2599\n",
      "Topic 570: 0.4219\n",
      "Topic 571: 0.2823\n",
      "Topic 572: 0.3588\n",
      "Topic 573: 0.3155\n",
      "Topic 574: 0.8186\n",
      "Topic 575: 0.5715\n",
      "Topic 576: 0.3492\n",
      "Topic 577: 0.3643\n",
      "Topic 578: 0.4458\n",
      "Topic 579: 0.6392\n",
      "Topic 580: 0.5503\n",
      "Topic 581: 0.3780\n",
      "Topic 582: 0.3814\n",
      "Topic 583: 0.6077\n",
      "Topic 584: 0.3074\n",
      "Topic 585: 0.3986\n",
      "Topic 586: 0.5638\n",
      "Topic 587: 0.6428\n",
      "Topic 588: 0.3276\n",
      "Topic 589: 0.3332\n",
      "Topic 590: 0.7100\n",
      "Topic 591: 0.2984\n",
      "Topic 592: 0.3722\n",
      "Topic 593: 0.7027\n",
      "Topic 594: 0.6059\n",
      "Topic 595: 0.5700\n",
      "Topic 596: 0.4353\n",
      "Topic 597: 0.3572\n",
      "Topic 598: 0.2863\n",
      "Topic 599: 0.3503\n",
      "Topic 600: 0.2779\n",
      "Topic 601: 0.3070\n",
      "Topic 602: 0.3114\n",
      "Topic 603: 0.5203\n",
      "Topic 604: 0.3803\n",
      "Topic 605: 0.2524\n",
      "Topic 606: 0.4004\n",
      "Topic 607: 0.4127\n",
      "Topic 608: 0.4392\n",
      "Topic 609: 0.2896\n",
      "Topic 610: 0.4867\n",
      "Topic 611: 0.2766\n",
      "Topic 612: 0.3432\n",
      "Topic 613: 0.3048\n",
      "Topic 614: 0.3467\n",
      "Topic 615: 0.5672\n",
      "Topic 616: 0.5066\n",
      "Topic 617: 0.4224\n",
      "Topic 618: 0.3098\n",
      "Topic 619: 0.2249\n",
      "Topic 620: 0.3310\n",
      "Topic 621: 0.4162\n",
      "Topic 622: 0.3094\n",
      "Topic 623: 0.2533\n",
      "Topic 624: 0.5023\n",
      "Topic 625: 0.3424\n",
      "Topic 626: 0.3236\n",
      "Topic 627: 0.3462\n",
      "Topic 628: 0.4044\n",
      "Topic 629: 0.3431\n",
      "Topic 630: 0.2801\n",
      "Topic 631: 0.3066\n",
      "Topic 632: 0.3394\n",
      "Topic 633: 0.3784\n",
      "Topic 634: 0.2881\n",
      "Topic 635: 0.4266\n",
      "Topic 636: 0.4775\n",
      "Topic 637: 0.3038\n",
      "Topic 638: 0.2349\n",
      "Topic 639: 0.2934\n",
      "Topic 640: 0.6948\n",
      "Topic 641: 0.3797\n",
      "Topic 642: 0.2236\n",
      "Topic 643: 0.3830\n",
      "Topic 644: 0.6453\n",
      "Topic 645: 0.4035\n",
      "Topic 646: 0.2759\n",
      "Topic 647: 0.6774\n",
      "Topic 648: 0.4038\n",
      "Topic 649: 0.4218\n",
      "Topic 650: 0.2781\n",
      "Topic 651: 0.3138\n",
      "Topic 652: 0.2857\n",
      "Topic 653: 0.3670\n",
      "Topic 654: 0.3919\n",
      "Topic 655: 0.3205\n",
      "Topic 656: 0.2963\n",
      "Topic 657: 0.2191\n",
      "Topic 658: 0.3330\n",
      "Topic 659: 0.2725\n",
      "Topic 660: 0.2724\n",
      "Topic 661: 0.2791\n",
      "Topic 662: 0.4407\n",
      "Topic 663: 0.3692\n",
      "Topic 664: 0.2420\n",
      "Topic 665: 0.2355\n",
      "Topic 666: 0.4183\n",
      "Topic 667: 0.6568\n",
      "Topic 668: 0.3648\n",
      "Topic 669: 0.5504\n",
      "Topic 670: 0.3762\n",
      "Topic 671: 0.3232\n",
      "Topic 672: 0.5271\n",
      "Topic 673: 0.3796\n",
      "Topic 674: 0.3685\n",
      "Topic 675: 0.4268\n",
      "Topic 676: 0.3846\n",
      "Topic 677: 0.2898\n",
      "Topic 678: 0.3543\n",
      "Topic 679: 0.3368\n",
      "Topic 680: 0.3060\n",
      "Topic 681: 0.2854\n",
      "Topic 682: 0.8301\n",
      "Topic 683: 1.0000\n",
      "Topic 684: 0.2539\n",
      "Topic 685: 0.4647\n",
      "Topic 686: 0.2854\n",
      "Topic 687: 1.0000\n",
      "Topic 688: 0.2391\n",
      "Topic 689: 0.4412\n",
      "Topic 690: 0.5372\n",
      "Topic 691: 0.2693\n",
      "Topic 692: 0.2559\n",
      "Topic 693: 0.4256\n",
      "Topic 694: 0.3403\n",
      "Topic 695: 0.2424\n",
      "Topic 696: 0.2681\n",
      "Topic 697: 0.3262\n",
      "Topic 698: 0.4372\n",
      "Topic 699: 0.2356\n",
      "Topic 700: 0.3704\n",
      "Topic 701: 0.3391\n",
      "Topic 702: 0.2721\n",
      "Topic 703: 0.4056\n",
      "Topic 704: 0.4078\n",
      "Topic 705: 0.2583\n",
      "Topic 706: 0.6104\n",
      "Topic 707: 0.2885\n",
      "Topic 708: 0.3815\n",
      "Topic 709: 0.2897\n",
      "Topic 710: 0.3315\n",
      "Topic 711: 0.3706\n",
      "Topic 712: 0.2958\n",
      "Topic 713: 0.4777\n",
      "Topic 714: 0.2590\n",
      "Topic 715: 0.4209\n",
      "Topic 716: 0.3262\n",
      "Topic 717: 0.2054\n",
      "Topic 718: 0.3605\n",
      "Topic 719: 0.3780\n",
      "Topic 720: 0.4780\n",
      "Topic 721: 0.3186\n",
      "Topic 722: 0.2824\n",
      "Topic 723: 0.1614\n",
      "Topic 724: 0.4346\n",
      "Topic 725: 0.2401\n",
      "Topic 726: 0.2438\n",
      "Topic 727: 0.2857\n",
      "Topic 728: 0.4205\n",
      "Topic 729: 0.3445\n",
      "Topic 730: 0.3423\n",
      "Topic 731: 0.5938\n",
      "Topic 732: 0.2754\n",
      "Topic 733: 0.3446\n",
      "Topic 734: 0.3081\n",
      "Topic 735: 0.2902\n",
      "Topic 736: 0.3399\n",
      "Topic 737: 0.2846\n",
      "Topic 738: 0.2914\n",
      "Topic 739: 0.2220\n",
      "Topic 740: 0.3612\n",
      "Topic 741: 0.2550\n",
      "Topic 742: 0.3147\n",
      "Topic 743: 0.3979\n",
      "Topic 744: 0.4703\n",
      "Topic 745: 0.2868\n",
      "Topic 746: 0.3374\n",
      "Topic 747: 0.5318\n",
      "Topic 748: 0.2924\n",
      "Topic 749: 0.3532\n",
      "Topic 750: 0.2882\n",
      "Topic 751: 0.3911\n",
      "Topic 752: 0.4482\n",
      "Topic 753: 0.2534\n",
      "Topic 754: 0.3636\n",
      "Topic 755: 0.2599\n",
      "Topic 756: 0.3247\n",
      "Topic 757: 0.2658\n",
      "Topic 758: 0.3830\n",
      "Topic 759: 0.3447\n",
      "Topic 760: 0.6789\n",
      "Topic 761: 0.3783\n",
      "Topic 762: 0.2436\n",
      "Topic 763: 0.4120\n",
      "Topic 764: 0.3547\n",
      "Topic 765: 0.6053\n",
      "Topic 766: 0.7751\n",
      "Topic 767: 0.2800\n",
      "Topic 768: 0.8408\n",
      "Topic 769: 0.5206\n",
      "Topic 770: 0.3373\n",
      "Topic 771: 0.2513\n",
      "Topic 772: 0.2927\n",
      "Topic 773: 0.5433\n",
      "Topic 774: 0.3491\n",
      "Topic 775: 0.3711\n",
      "Topic 776: 0.4037\n",
      "Topic 777: 0.2476\n",
      "Topic 778: 0.5250\n",
      "Topic 779: 0.2377\n",
      "Topic 780: 0.3150\n",
      "Topic 781: 0.2590\n",
      "Topic 782: 0.5416\n",
      "Topic 783: 0.3710\n",
      "Topic 784: 0.2399\n",
      "Topic 785: 0.4593\n",
      "Topic 786: 0.4649\n",
      "Topic 787: 0.6843\n",
      "Topic 788: 0.4544\n",
      "Topic 789: 0.4229\n",
      "Topic 790: 0.2585\n",
      "Topic 791: 0.3419\n",
      "Topic 792: 0.2627\n",
      "Topic 793: 0.4297\n",
      "Topic 794: 0.3660\n",
      "Topic 795: 0.5956\n",
      "Topic 796: 0.3058\n",
      "Topic 797: 0.3189\n",
      "Topic 798: 0.3244\n",
      "Topic 799: 0.3021\n",
      "Topic 800: 0.4979\n",
      "Topic 801: 0.5208\n",
      "Topic 802: 0.3903\n",
      "Topic 803: 0.2904\n",
      "Topic 804: 0.6286\n",
      "Topic 805: 0.8968\n",
      "Topic 806: 0.2666\n",
      "Topic 807: 0.3497\n",
      "Topic 808: 0.2209\n",
      "Topic 809: 0.7017\n",
      "Topic 810: 0.2371\n",
      "Topic 811: 0.3719\n",
      "Topic 812: 0.3148\n",
      "Topic 813: 0.2693\n",
      "Topic 814: 0.4517\n",
      "Topic 815: 0.2556\n",
      "Topic 816: 0.3263\n",
      "Topic 817: 0.4196\n",
      "Topic 818: 0.3670\n",
      "Topic 819: 0.5529\n",
      "Topic 820: 0.4329\n",
      "Topic 821: 0.6299\n",
      "Topic 822: 0.2612\n",
      "Topic 823: 0.5207\n",
      "Topic 824: 0.2511\n",
      "Topic 825: 0.3870\n",
      "Topic 826: 0.3591\n",
      "Topic 827: 0.3131\n",
      "Topic 828: 0.3807\n",
      "Topic 829: 0.6060\n",
      "Topic 830: 0.3420\n",
      "Topic 831: 0.3705\n",
      "Topic 832: 0.3313\n",
      "Topic 833: 0.3241\n",
      "Topic 834: 0.2833\n",
      "Topic 835: 0.3316\n",
      "Topic 836: 0.4330\n",
      "Topic 837: 0.2804\n",
      "Topic 838: 0.5233\n",
      "Topic 839: 0.3053\n",
      "Topic 840: 0.2896\n",
      "Topic 841: 0.4810\n",
      "Topic 842: 0.8253\n",
      "Topic 843: 0.4092\n",
      "Topic 844: 0.3020\n",
      "Topic 845: 0.3545\n",
      "Topic 846: 0.3729\n",
      "Topic 847: 0.5320\n",
      "Topic 848: 0.3495\n",
      "Topic 849: 0.3400\n",
      "Topic 850: 0.2504\n",
      "Topic 851: 0.4639\n",
      "Topic 852: 0.5602\n",
      "Topic 853: 0.2532\n",
      "Topic 854: 0.2935\n",
      "Topic 855: 0.2959\n",
      "Topic 856: 0.3155\n",
      "Topic 857: 0.2503\n",
      "Topic 858: 0.6015\n",
      "Topic 859: 0.2827\n",
      "Topic 860: 0.3436\n",
      "Topic 861: 0.4582\n",
      "Topic 862: 0.4468\n",
      "Topic 863: 0.4159\n",
      "Topic 864: 0.3560\n",
      "Topic 865: 0.8343\n",
      "Topic 866: 0.2468\n",
      "Topic 867: 0.3758\n",
      "Topic 868: 0.4304\n",
      "Topic 869: 0.6524\n",
      "Topic 870: 0.3303\n",
      "Topic 871: 0.3904\n",
      "Topic 872: 0.3518\n",
      "Topic 873: 0.3214\n",
      "Topic 874: 0.2282\n",
      "Topic 875: 0.3468\n",
      "Topic 876: 0.3367\n",
      "Topic 877: 0.3030\n",
      "Topic 878: 0.4630\n",
      "Topic 879: 0.3136\n",
      "Topic 880: 0.4165\n",
      "Topic 881: 0.3123\n",
      "Topic 882: 0.4003\n",
      "Topic 883: 0.3548\n",
      "Topic 884: 0.5310\n",
      "Topic 885: 0.2160\n",
      "Topic 886: 0.2674\n",
      "Topic 887: 0.3418\n",
      "Topic 888: 0.2605\n",
      "Topic 889: 0.2647\n",
      "Topic 890: 0.3589\n",
      "Topic 891: 0.7470\n",
      "Topic 892: 0.3364\n",
      "Topic 893: 0.3368\n",
      "Topic 894: 0.2881\n",
      "Topic 895: 0.2614\n",
      "Topic 896: 0.8875\n",
      "Topic 897: 0.2417\n",
      "Topic 898: 0.3296\n",
      "Topic 899: 0.4452\n",
      "Topic 900: 0.3475\n",
      "Topic 901: 0.2714\n",
      "Topic 902: 0.4336\n",
      "Topic 903: 0.2564\n",
      "Topic 904: 0.3122\n",
      "Topic 905: 0.2650\n",
      "Topic 906: 0.3759\n",
      "Topic 907: 0.4181\n",
      "Topic 908: 0.5672\n",
      "Topic 909: 0.3949\n",
      "Topic 910: 0.2608\n",
      "Topic 911: 0.2644\n",
      "Topic 912: 0.2767\n",
      "Topic 913: 0.7120\n",
      "Topic 914: 0.4294\n",
      "Topic 915: 0.3820\n",
      "Topic 916: 0.2828\n",
      "Topic 917: 0.2662\n",
      "Topic 918: 0.3402\n",
      "Topic 919: 0.3038\n",
      "Topic 920: 0.3015\n",
      "Topic 921: 0.1848\n",
      "Topic 922: 0.2269\n",
      "Topic 923: 0.4458\n",
      "Topic 924: 0.3526\n",
      "Topic 925: 0.3818\n",
      "Topic 926: 0.3419\n",
      "Topic 927: 0.1902\n",
      "Topic 928: 0.4031\n",
      "Topic 929: 0.5268\n",
      "Topic 930: 0.5440\n",
      "Topic 931: 0.5575\n",
      "Topic 932: 0.3483\n",
      "Topic 933: 0.3925\n",
      "Topic 934: 0.3529\n",
      "Topic 935: 0.3681\n",
      "Topic 936: 0.3203\n",
      "Topic 937: 0.2810\n",
      "Topic 938: 0.3115\n",
      "Topic 939: 0.3088\n",
      "Topic 940: 0.3245\n",
      "Topic 941: 0.2657\n",
      "Topic 942: 0.3701\n",
      "Topic 943: 0.2573\n",
      "Topic 944: 0.4255\n",
      "Topic 945: 0.3633\n",
      "Topic 946: 0.4420\n",
      "Topic 947: 0.3249\n",
      "Topic 948: 0.2835\n",
      "Topic 949: 0.2375\n",
      "Topic 950: 0.2821\n",
      "Topic 951: 0.3595\n",
      "Topic 952: 0.3167\n",
      "Topic 953: 0.4028\n",
      "Topic 954: 0.2774\n",
      "Topic 955: 0.2214\n",
      "Topic 956: 0.2861\n",
      "Topic 957: 0.5950\n",
      "Topic 958: 0.7177\n",
      "Topic 959: 0.3676\n",
      "Topic 960: 0.2797\n",
      "Topic 961: 0.4090\n",
      "Topic 962: 0.2387\n",
      "Topic 963: 0.3526\n",
      "Topic 964: 0.2447\n",
      "Topic 965: 0.2280\n",
      "Topic 966: 0.5677\n",
      "Topic 967: 0.4210\n",
      "Topic 968: 0.3502\n",
      "Topic 969: 0.2744\n",
      "Topic 970: 0.7409\n",
      "Topic 971: 0.2512\n",
      "Topic 972: 0.4673\n",
      "Topic 973: 0.3633\n",
      "Topic 974: 0.4891\n",
      "Topic 975: 0.3325\n",
      "Topic 976: 0.3608\n",
      "Topic 977: 0.2102\n",
      "Topic 978: 0.3527\n",
      "Topic 979: 0.3000\n",
      "Topic 980: 0.4920\n",
      "Topic 981: 0.2538\n",
      "Topic 982: 0.3769\n",
      "Topic 983: 0.3209\n",
      "Topic 984: 0.2908\n",
      "Topic 985: 0.3502\n",
      "Topic 986: 0.3791\n",
      "Topic 987: 0.2345\n",
      "Topic 988: 0.3181\n",
      "Topic 989: 0.4100\n",
      "Topic 990: 0.3123\n",
      "Topic 991: 0.3549\n",
      "Topic 992: 0.4870\n",
      "Topic 993: 0.2486\n",
      "Topic 994: 0.3964\n",
      "Topic 995: 0.3941\n",
      "Topic 996: 0.2605\n",
      "Topic 997: 0.1537\n",
      "Topic 998: 0.2798\n",
      "Topic 999: 0.4876\n",
      "Topic 1000: 0.2726\n",
      "Topic 1001: 0.3284\n",
      "Topic 1002: 0.5116\n",
      "Topic 1003: 0.3221\n",
      "Topic 1004: 0.3625\n",
      "Topic 1005: 0.4980\n",
      "Topic 1006: 0.3396\n",
      "Topic 1007: 0.3966\n",
      "Topic 1008: 0.2325\n",
      "Topic 1009: 0.3101\n",
      "Topic 1010: 0.3243\n",
      "Topic 1011: 0.1971\n",
      "Topic 1012: 0.3240\n",
      "Topic 1013: 0.5828\n",
      "Topic 1014: 0.3627\n",
      "Topic 1015: 0.2575\n",
      "Topic 1016: 0.3163\n",
      "Topic 1017: 0.4351\n",
      "Topic 1018: 0.3293\n",
      "Topic 1019: 0.2800\n",
      "Topic 1020: 0.3829\n",
      "Topic 1021: 0.5484\n",
      "Topic 1022: 0.3197\n",
      "Topic 1023: 0.3333\n",
      "Topic 1024: 0.8241\n",
      "Topic 1025: 0.3279\n",
      "Topic 1026: 1.0000\n",
      "Topic 1027: 0.5749\n",
      "Topic 1028: 0.2942\n",
      "Topic 1029: 0.3756\n",
      "Topic 1030: 0.3164\n",
      "Topic 1031: 0.2649\n",
      "Topic 1032: 0.3504\n",
      "Topic 1033: 0.2402\n",
      "Topic 1034: 0.2263\n",
      "Topic 1035: 0.2883\n",
      "Topic 1036: 0.3356\n",
      "Topic 1037: 0.3264\n",
      "Topic 1038: 0.4424\n",
      "Topic 1039: 0.3485\n",
      "Topic 1040: 0.3459\n",
      "Topic 1041: 0.4407\n",
      "Topic 1042: 0.2874\n",
      "Topic 1043: 0.5145\n",
      "Topic 1044: 0.3492\n",
      "Topic 1045: 0.3006\n",
      "Topic 1046: 0.6853\n",
      "Topic 1047: 0.3841\n",
      "Topic 1048: 0.3008\n",
      "Topic 1049: 0.2974\n",
      "Topic 1050: 0.3320\n",
      "Topic 1051: 0.3828\n",
      "Topic 1052: 0.5015\n",
      "Topic 1054: 0.2704\n",
      "Topic 1055: 0.3473\n",
      "Topic 1056: 0.2500\n",
      "Topic 1057: 0.3204\n",
      "Topic 1058: 0.3544\n",
      "Topic 1059: 0.3251\n",
      "Topic 1060: 0.2432\n",
      "Topic 1061: 0.2680\n",
      "Topic 1062: 0.2355\n",
      "Topic 1063: 0.5766\n",
      "Topic 1064: 0.4416\n",
      "Topic 1065: 0.4137\n",
      "Topic 1066: 0.5439\n",
      "Topic 1067: 0.2960\n",
      "Topic 1068: 0.3613\n",
      "Topic 1069: 0.3634\n",
      "Topic 1070: 0.5998\n",
      "Topic 1071: 0.3334\n",
      "Topic 1072: 0.3146\n",
      "Topic 1073: 0.6282\n",
      "Topic 1074: 0.2668\n",
      "Topic 1075: 0.2539\n",
      "Topic 1076: 0.3121\n",
      "Topic 1077: 0.2663\n",
      "Topic 1078: 0.4388\n",
      "Topic 1079: 0.2809\n",
      "Topic 1080: 0.3670\n",
      "Topic 1081: 0.2722\n",
      "Topic 1082: 0.4669\n",
      "Topic 1083: 0.4154\n",
      "Topic 1084: 0.5830\n",
      "Topic 1085: 0.2426\n",
      "Topic 1086: 0.3207\n",
      "Topic 1087: 0.3164\n",
      "Topic 1088: 0.6843\n",
      "Topic 1089: 0.5740\n",
      "Topic 1090: 0.2324\n",
      "Topic 1091: 0.1525\n",
      "Topic 1092: 0.2643\n",
      "Topic 1093: 0.4354\n",
      "Topic 1094: 0.5216\n",
      "Topic 1095: 0.2629\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating overall coherence score...\")\n",
    "coherence_score = calculate_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "\n",
    "if coherence_score is not None:\n",
    "    print(f\"\\nTopic Coherence Score (C_v): {coherence_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate overall coherence score\")\n",
    "\n",
    "# Calculate and print per-topic coherence scores\n",
    "print(\"\\nCalculating per-topic coherence scores...\")\n",
    "per_topic_scores = calculate_per_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "print(\"\\nPer-topic coherence scores:\")\n",
    "for topic, score in per_topic_scores.items():\n",
    "    if score is not None:\n",
    "        print(f\"Topic {topic}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"Topic {topic}: Could not calculate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T21:48:40.498671Z",
     "iopub.status.busy": "2024-10-26T21:48:40.498191Z",
     "iopub.status.idle": "2024-10-26T21:52:12.978680Z",
     "shell.execute_reply": "2024-10-26T21:52:12.977433Z",
     "shell.execute_reply.started": "2024-10-26T21:48:40.498627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Topics:\n",
      "    Topic  Count                                              Name  \\\n",
      "0      -1  36868      -1_singaporeans_singapore_government_support   \n",
      "1       0   3174                       0_asians_asian_racial_races   \n",
      "2       1   1498         1_interviews_interview_interviewed_hiring   \n",
      "3       2   1028                      2_fat_diet_unhealthy_healthy   \n",
      "4       3    821                        3_roads_traffic_buses_road   \n",
      "5       4    708                                          4_yes___   \n",
      "6       5    693                        5_covid_pandemic_flu_virus   \n",
      "7       6    548                    6_fight_fighting_fights_combat   \n",
      "8       7    533                   7_thank_thanks_welcome_grateful   \n",
      "9       8    424               8_parents_parent_families_childhood   \n",
      "10      9    411                    9_courts_accused_justice_court   \n",
      "11     10    396           10_trolls_insult_insulting_conversation   \n",
      "12     11    391                 11_drugs_drug_medication_medicine   \n",
      "13     12    347                   12_spouse_husband_wife_marriage   \n",
      "14     13    341                    13_sleep_asleep_sleeping_slept   \n",
      "15     14    335                       14_passport_ica_visa_status   \n",
      "16     15    332                      15_hilarious_funny_lol_lolol   \n",
      "17     16    301                              16_sia_sian_siao_nia   \n",
      "18     17    287                        17_tesla_cars_vehicles_car   \n",
      "19     18    287  18_pregnancies_pregnancy_pregnant_discrimination   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [singaporeans, singapore, government, support,...   \n",
      "1   [asians, asian, racial, races, racism, singapo...   \n",
      "2   [interviews, interview, interviewed, hiring, j...   \n",
      "3   [fat, diet, unhealthy, healthy, fitness, healt...   \n",
      "4   [roads, traffic, buses, road, bus, lane, trans...   \n",
      "5                             [yes, , , , , , , , , ]   \n",
      "6   [covid, pandemic, flu, virus, fever, diseases,...   \n",
      "7   [fight, fighting, fights, combat, fighter, kic...   \n",
      "8   [thank, thanks, welcome, grateful, glad, appre...   \n",
      "9   [parents, parent, families, childhood, grandpa...   \n",
      "10  [courts, accused, justice, court, judgment, ju...   \n",
      "11  [trolls, insult, insulting, conversation, trol...   \n",
      "12  [drugs, drug, medication, medicine, treatment,...   \n",
      "13  [spouse, husband, wife, marriage, married, mar...   \n",
      "14  [sleep, asleep, sleeping, slept, waking, woke,...   \n",
      "15  [passport, ica, visa, status, immigration, app...   \n",
      "16  [hilarious, funny, lol, lolol, lmao, funniest,...   \n",
      "17  [sia, sian, siao, nia, mai, liao, kio, bishan,...   \n",
      "18  [tesla, cars, vehicles, car, vehicle, driven, ...   \n",
      "19  [pregnancies, pregnancy, pregnant, discriminat...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [different context political backdrop realize ...  \n",
      "1   [blackpill communityif whte man sea girls sout...  \n",
      "2   [ghosted quite frequently interviewing current...  \n",
      "3   [fat healthy ask pushups anyway feel bullying ...  \n",
      "4   [most part everyday walks come across few dang...  \n",
      "5         [yes yes yes, yes yes yes yes, yes yes yes]  \n",
      "6   [week week increase covid community cases driv...  \n",
      "7   [yes fight fight fight where police, better fi...  \n",
      "8   [thank much, thank, thank much whatever manage...  \n",
      "9   [rarely hear positive stories kids grew low in...  \n",
      "10  [think point making just going use initials wh...  \n",
      "11  [only replying nope replying other guy too cho...  \n",
      "12  [except drug dealers execute, like mentioned s...  \n",
      "13  [judge noted husband later deposited million o...  \n",
      "14  [still awake please sleep, can sleep, can slee...  \n",
      "15  [validity old passport cannot added https www ...  \n",
      "16      [lol hilarious, lol hilarious, lol hilarious]  \n",
      "17                      [how find sia, sia, same sia]  \n",
      "18  [really drink kool aid har market too small ma...  \n",
      "19  [don think issue here women getting pregnant d...  \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df = pd.read_csv('/kaggle/input/combined-data-scores/combined_data_scores.csv')\n",
    "yearmonth = '2022-06'\n",
    "df_sample = df[df['yearmonth'] == yearmonth]\n",
    "\n",
    "# Preprocess the texts\n",
    "preprocessed_texts = preprocess_text(df_sample['text'])\n",
    "\n",
    "# Initialize BERTopic model\n",
    "representation_model = KeyBERTInspired()\n",
    "vectorizer_model = CountVectorizer(min_df=10, stop_words=\"english\")\n",
    "topic_model = BERTopic(\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=\"auto\",\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probabilities = topic_model.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Get topic info\n",
    "print(\"\\nTop 20 Topics:\")\n",
    "print(topic_model.get_topic_info().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T21:53:10.342860Z",
     "iopub.status.busy": "2024-10-26T21:53:10.342003Z",
     "iopub.status.idle": "2024-10-26T21:56:18.377546Z",
     "shell.execute_reply": "2024-10-26T21:56:18.376544Z",
     "shell.execute_reply.started": "2024-10-26T21:53:10.342813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating overall coherence score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Coherence Score (C_v): 0.3792\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating overall coherence score...\")\n",
    "coherence_score = calculate_topic_coherence(topic_model, preprocessed_texts, topics)\n",
    "\n",
    "if coherence_score is not None:\n",
    "    print(f\"\\nTopic Coherence Score (C_v): {coherence_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate overall coherence score\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5935933,
     "sourceId": 9705720,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5936031,
     "sourceId": 9705856,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5939253,
     "sourceId": 9710067,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
